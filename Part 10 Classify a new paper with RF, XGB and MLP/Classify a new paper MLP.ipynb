{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In Part 9 KMeans Clustering vs LDA Topic Modeling. we did topic modeling on the 1941 papers mentioned Polymerase.\n",
    "\n",
    "Topic Modeling helped us quickly gain the big picture of the major topics in a large collection of papers. It also helps by giving a rough index so you know where to look for the type of information you want (which cluster you should go). For example, if you are looking for information about diagnosis assay, you should probably go to \"Group 5 diagnosis assay detection\" to find information faster. If you are interested about \"children\" and \"clinical\" related information, you should go to \"Group 1 flu children clinical\".\n",
    "\n",
    "\n",
    "The topics identified as below:\n",
    "### Topics:\n",
    "#### G0 flu children clinical\n",
    "#### G1 gene protein cell \n",
    "#### G2 bat host phylogenetic \n",
    "#### G3 SARS COV MERS COV\n",
    "#### G4 diganosis asssay detection \n",
    "#### G5 RdRp RNA replication\n",
    "\n",
    "\n",
    "## Method\n",
    "#### Use these topics as labels, we can use the 1941 papers (subset on March 26) as a training dataset to build a classification model that classify new papers. I use 388 papers newly updated papers (subet of April 12-subset on March 26) as a test dataset to give these new papers a topic label.\n",
    "\n",
    "### Steps:\n",
    "#### Use Universal Sentence Encoder (USE) to vectorize abstracts to 512 dimension vectors.\n",
    "#### Use the G0-5 as the true labels to train classifers.\n",
    "#### Validate each classifier achieved good performance.\n",
    "#### Majority vote of RF, GXB, MLP to generate the final prediction.\n",
    "\n",
    "## Results\n",
    "XGB predictions match well with MLP predictions with 89.4% agreement.\n",
    "\n",
    "XGB vs RF: agree on 81.2% of the predictions\n",
    "\n",
    "MLP vs RF: agree on 81.4% of the predictions\n",
    "\n",
    "There are 10 instances that all three methods produce different predictions. So the majority vote would not work. For a problem like this with 6 classes, we need at least 7 different models to ensemble to make sure there is a majority vote result.\n",
    "\n",
    "\n",
    "## Future work\n",
    "#### To improve the models, consider it is a imbalance dataset use class weight.\n",
    "\n",
    "\n",
    "## Code\n",
    "#### The code for prepare the data and other classifiers are on GitHub:\n",
    "#### https://github.com/lj89/CORD19/tree/master/Part%2010%20Classify%20a%20new%20paper%20with%20RF%2C%20XGB%20and%20MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import  SimpleRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;print(\"tensorflow:\",tf.__version__)\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>Group</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Selective induction of interleukin-6 in mouse ...</td>\n",
       "      <td>Abstract Astrocytes produce granulocyte/macrop...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024093</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.026175</td>\n",
       "      <td>-0.056424</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.064344</td>\n",
       "      <td>-0.015630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030333</td>\n",
       "      <td>-0.068542</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.068533</td>\n",
       "      <td>-0.047693</td>\n",
       "      <td>-0.045203</td>\n",
       "      <td>0.038434</td>\n",
       "      <td>-0.052579</td>\n",
       "      <td>-0.026946</td>\n",
       "      <td>-0.064774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The coronavirus avian infectious bronchitis vi...</td>\n",
       "      <td>Abstract Replication of avian infectious bronc...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.015660</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>-0.023342</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064577</td>\n",
       "      <td>-0.065933</td>\n",
       "      <td>-0.046359</td>\n",
       "      <td>-0.065978</td>\n",
       "      <td>-0.057970</td>\n",
       "      <td>-0.046468</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>-0.053377</td>\n",
       "      <td>-0.034778</td>\n",
       "      <td>0.007275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Effect of sulfhydryl reagents on the infectivi...</td>\n",
       "      <td>Abstract The infectivity of vesicular stomatit...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.029008</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>-0.066768</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>0.067563</td>\n",
       "      <td>0.040172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009418</td>\n",
       "      <td>-0.068801</td>\n",
       "      <td>-0.007311</td>\n",
       "      <td>-0.068773</td>\n",
       "      <td>-0.016543</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>0.041948</td>\n",
       "      <td>-0.049344</td>\n",
       "      <td>-0.005887</td>\n",
       "      <td>-0.032376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Further characterization of mouse hepatitis vi...</td>\n",
       "      <td>Abstract Two temporally and enzymatically dist...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.025092</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.036179</td>\n",
       "      <td>-0.007282</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.047946</td>\n",
       "      <td>0.032226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057155</td>\n",
       "      <td>-0.065491</td>\n",
       "      <td>-0.047325</td>\n",
       "      <td>-0.065456</td>\n",
       "      <td>-0.027758</td>\n",
       "      <td>-0.018844</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>-0.059728</td>\n",
       "      <td>-0.057004</td>\n",
       "      <td>-0.043818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Characterization of Kunjin virus RNA-dependent...</td>\n",
       "      <td>Abstract RNA-dependent RNA polymerase (RDRP) a...</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.020635</td>\n",
       "      <td>0.030479</td>\n",
       "      <td>-0.051886</td>\n",
       "      <td>-0.055125</td>\n",
       "      <td>-0.052440</td>\n",
       "      <td>0.051107</td>\n",
       "      <td>-0.022648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028083</td>\n",
       "      <td>-0.061142</td>\n",
       "      <td>-0.018120</td>\n",
       "      <td>-0.061137</td>\n",
       "      <td>-0.049363</td>\n",
       "      <td>-0.026236</td>\n",
       "      <td>0.053327</td>\n",
       "      <td>-0.014145</td>\n",
       "      <td>-0.046835</td>\n",
       "      <td>-0.060753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 515 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Selective induction of interleukin-6 in mouse ...   \n",
       "1  The coronavirus avian infectious bronchitis vi...   \n",
       "2  Effect of sulfhydryl reagents on the infectivi...   \n",
       "3  Further characterization of mouse hepatitis vi...   \n",
       "4  Characterization of Kunjin virus RNA-dependent...   \n",
       "\n",
       "                                            abstract  Group         0  \\\n",
       "0  Abstract Astrocytes produce granulocyte/macrop...      1  0.024093   \n",
       "1  Abstract Replication of avian infectious bronc...      5  0.000738   \n",
       "2  Abstract The infectivity of vesicular stomatit...      1 -0.029008   \n",
       "3  Abstract Two temporally and enzymatically dist...      5  0.025092   \n",
       "4  Abstract RNA-dependent RNA polymerase (RDRP) a...      5 -0.020635   \n",
       "\n",
       "          1         2         3         4         5         6  ...       502  \\\n",
       "0  0.003332  0.026175 -0.056424  0.066987  0.064344 -0.015630  ... -0.030333   \n",
       "1  0.028788  0.008383  0.015660 -0.002792 -0.023342  0.039279  ... -0.064577   \n",
       "2  0.036773  0.016296 -0.066768  0.026380  0.067563  0.040172  ... -0.009418   \n",
       "3 -0.005877  0.036179 -0.007282  0.044100  0.047946  0.032226  ... -0.057155   \n",
       "4  0.030479 -0.051886 -0.055125 -0.052440  0.051107 -0.022648  ... -0.028083   \n",
       "\n",
       "        503       504       505       506       507       508       509  \\\n",
       "0 -0.068542  0.004144 -0.068533 -0.047693 -0.045203  0.038434 -0.052579   \n",
       "1 -0.065933 -0.046359 -0.065978 -0.057970 -0.046468  0.047742 -0.053377   \n",
       "2 -0.068801 -0.007311 -0.068773 -0.016543  0.005688  0.041948 -0.049344   \n",
       "3 -0.065491 -0.047325 -0.065456 -0.027758 -0.018844  0.033387 -0.059728   \n",
       "4 -0.061142 -0.018120 -0.061137 -0.049363 -0.026236  0.053327 -0.014145   \n",
       "\n",
       "        510       511  \n",
       "0 -0.026946 -0.064774  \n",
       "1 -0.034778  0.007275  \n",
       "2 -0.005887 -0.032376  \n",
       "3 -0.057004 -0.043818  \n",
       "4 -0.046835 -0.060753  \n",
       "\n",
       "[5 rows x 515 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'C:/Users/N1110/Desktop/SMU 2020 SPRING/CORD19/april 15 afternoon/'\n",
    "df1= pd.read_csv('Train1941_512vectorsWithTitles.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 515)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        object\n",
       "abstract     object\n",
       "Group         int64\n",
       "0           float64\n",
       "1           float64\n",
       "             ...   \n",
       "507         float64\n",
       "508         float64\n",
       "509         float64\n",
       "510         float64\n",
       "511         float64\n",
       "Length: 515, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.112313</td>\n",
       "      <td>-0.033354</td>\n",
       "      <td>0.025039</td>\n",
       "      <td>-0.029933</td>\n",
       "      <td>-0.025233</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.033986</td>\n",
       "      <td>0.030405</td>\n",
       "      <td>-0.025332</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041908</td>\n",
       "      <td>-0.062183</td>\n",
       "      <td>-0.022047</td>\n",
       "      <td>-0.062076</td>\n",
       "      <td>0.022722</td>\n",
       "      <td>-0.019891</td>\n",
       "      <td>0.033020</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.017537</td>\n",
       "      <td>-0.047506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.926152</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>0.034592</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>0.036274</td>\n",
       "      <td>0.032438</td>\n",
       "      <td>0.032549</td>\n",
       "      <td>0.031832</td>\n",
       "      <td>0.040776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.034609</td>\n",
       "      <td>0.034151</td>\n",
       "      <td>0.022946</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>0.020467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.073090</td>\n",
       "      <td>-0.064182</td>\n",
       "      <td>-0.067201</td>\n",
       "      <td>-0.076231</td>\n",
       "      <td>-0.067943</td>\n",
       "      <td>-0.063158</td>\n",
       "      <td>-0.073909</td>\n",
       "      <td>-0.066887</td>\n",
       "      <td>-0.074878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070765</td>\n",
       "      <td>-0.081105</td>\n",
       "      <td>-0.066845</td>\n",
       "      <td>-0.083125</td>\n",
       "      <td>-0.065105</td>\n",
       "      <td>-0.075116</td>\n",
       "      <td>-0.061486</td>\n",
       "      <td>-0.068634</td>\n",
       "      <td>-0.069124</td>\n",
       "      <td>-0.073304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056988</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>-0.052341</td>\n",
       "      <td>-0.049912</td>\n",
       "      <td>-0.009887</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>-0.051979</td>\n",
       "      <td>-0.028134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058623</td>\n",
       "      <td>-0.064305</td>\n",
       "      <td>-0.049817</td>\n",
       "      <td>-0.064194</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-0.049479</td>\n",
       "      <td>0.021046</td>\n",
       "      <td>-0.057128</td>\n",
       "      <td>-0.047880</td>\n",
       "      <td>-0.059254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.046209</td>\n",
       "      <td>0.036926</td>\n",
       "      <td>-0.039104</td>\n",
       "      <td>-0.035005</td>\n",
       "      <td>0.027843</td>\n",
       "      <td>0.048174</td>\n",
       "      <td>0.043775</td>\n",
       "      <td>-0.035518</td>\n",
       "      <td>0.017565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053042</td>\n",
       "      <td>-0.061603</td>\n",
       "      <td>-0.031897</td>\n",
       "      <td>-0.061564</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>-0.029447</td>\n",
       "      <td>0.038922</td>\n",
       "      <td>-0.051624</td>\n",
       "      <td>-0.027722</td>\n",
       "      <td>-0.056003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.015791</td>\n",
       "      <td>0.054346</td>\n",
       "      <td>-0.014463</td>\n",
       "      <td>-0.005188</td>\n",
       "      <td>0.047873</td>\n",
       "      <td>0.057508</td>\n",
       "      <td>0.055454</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033597</td>\n",
       "      <td>-0.059599</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-0.059553</td>\n",
       "      <td>0.051864</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.050912</td>\n",
       "      <td>-0.037323</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>-0.044880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.062857</td>\n",
       "      <td>0.072260</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>0.058647</td>\n",
       "      <td>0.067745</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.069029</td>\n",
       "      <td>0.062444</td>\n",
       "      <td>0.072816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054437</td>\n",
       "      <td>-0.053319</td>\n",
       "      <td>0.068436</td>\n",
       "      <td>-0.030560</td>\n",
       "      <td>0.071195</td>\n",
       "      <td>0.065397</td>\n",
       "      <td>0.064143</td>\n",
       "      <td>0.060514</td>\n",
       "      <td>0.063298</td>\n",
       "      <td>0.055845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Group            0            1            2            3  \\\n",
       "count  1941.000000  1941.000000  1941.000000  1941.000000  1941.000000   \n",
       "mean      2.112313    -0.033354     0.025039    -0.029933    -0.025233   \n",
       "std       1.926152     0.030242     0.034592     0.027972     0.030341   \n",
       "min       0.000000    -0.073090    -0.064182    -0.067201    -0.076231   \n",
       "25%       1.000000    -0.056988     0.002409    -0.052341    -0.049912   \n",
       "50%       1.000000    -0.046209     0.036926    -0.039104    -0.035005   \n",
       "75%       4.000000    -0.015791     0.054346    -0.014463    -0.005188   \n",
       "max       5.000000     0.062857     0.072260     0.064023     0.058647   \n",
       "\n",
       "                 4            5            6            7            8  ...  \\\n",
       "count  1941.000000  1941.000000  1941.000000  1941.000000  1941.000000  ...   \n",
       "mean      0.016948     0.033986     0.030405    -0.025332     0.009630  ...   \n",
       "std       0.036274     0.032438     0.032549     0.031832     0.040776  ...   \n",
       "min      -0.067943    -0.063158    -0.073909    -0.066887    -0.074878  ...   \n",
       "25%      -0.009887     0.020013     0.013695    -0.051979    -0.028134  ...   \n",
       "50%       0.027843     0.048174     0.043775    -0.035518     0.017565  ...   \n",
       "75%       0.047873     0.057508     0.055454    -0.004598     0.047810  ...   \n",
       "max       0.067745     0.072100     0.069029     0.062444     0.072816  ...   \n",
       "\n",
       "               502          503          504          505          506  \\\n",
       "count  1941.000000  1941.000000  1941.000000  1941.000000  1941.000000   \n",
       "mean     -0.041908    -0.062183    -0.022047    -0.062076     0.022722   \n",
       "std       0.025084     0.003800     0.033338     0.003946     0.034609   \n",
       "min      -0.070765    -0.081105    -0.066845    -0.083125    -0.065105   \n",
       "25%      -0.058623    -0.064305    -0.049817    -0.064194    -0.000385   \n",
       "50%      -0.053042    -0.061603    -0.031897    -0.061564     0.034213   \n",
       "75%      -0.033597    -0.059599     0.000451    -0.059553     0.051864   \n",
       "max       0.054437    -0.053319     0.068436    -0.030560     0.071195   \n",
       "\n",
       "               507          508          509          510          511  \n",
       "count  1941.000000  1941.000000  1941.000000  1941.000000  1941.000000  \n",
       "mean     -0.019891     0.033020    -0.043147    -0.017537    -0.047506  \n",
       "std       0.034151     0.022946     0.021903     0.035407     0.020467  \n",
       "min      -0.075116    -0.061486    -0.068634    -0.069124    -0.073304  \n",
       "25%      -0.049479     0.021046    -0.057128    -0.047880    -0.059254  \n",
       "50%      -0.029447     0.038922    -0.051624    -0.027722    -0.056003  \n",
       "75%       0.004885     0.050912    -0.037323     0.009025    -0.044880  \n",
       "max       0.065397     0.064143     0.060514     0.063298     0.055845  \n",
       "\n",
       "[8 rows x 513 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0\n",
       "abstract    0\n",
       "Group       0\n",
       "0           0\n",
       "1           0\n",
       "           ..\n",
       "507         0\n",
       "508         0\n",
       "509         0\n",
       "510         0\n",
       "511         0\n",
       "Length: 515, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "df1.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000002BD5EF9FA58>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.hist(column='Group', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.024093</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.026175</td>\n",
       "      <td>-0.056424</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.064344</td>\n",
       "      <td>-0.015630</td>\n",
       "      <td>-0.065082</td>\n",
       "      <td>0.063505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030333</td>\n",
       "      <td>-0.068542</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.068533</td>\n",
       "      <td>-0.047693</td>\n",
       "      <td>-0.045203</td>\n",
       "      <td>0.038434</td>\n",
       "      <td>-0.052579</td>\n",
       "      <td>-0.026946</td>\n",
       "      <td>-0.064774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.015660</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>-0.023342</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>-0.006778</td>\n",
       "      <td>-0.046869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064577</td>\n",
       "      <td>-0.065933</td>\n",
       "      <td>-0.046359</td>\n",
       "      <td>-0.065978</td>\n",
       "      <td>-0.057970</td>\n",
       "      <td>-0.046468</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>-0.053377</td>\n",
       "      <td>-0.034778</td>\n",
       "      <td>0.007275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.029008</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>-0.066768</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>0.067563</td>\n",
       "      <td>0.040172</td>\n",
       "      <td>-0.060464</td>\n",
       "      <td>-0.026711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009418</td>\n",
       "      <td>-0.068801</td>\n",
       "      <td>-0.007311</td>\n",
       "      <td>-0.068773</td>\n",
       "      <td>-0.016543</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>0.041948</td>\n",
       "      <td>-0.049344</td>\n",
       "      <td>-0.005887</td>\n",
       "      <td>-0.032376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.025092</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.036179</td>\n",
       "      <td>-0.007282</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.047946</td>\n",
       "      <td>0.032226</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.051182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057155</td>\n",
       "      <td>-0.065491</td>\n",
       "      <td>-0.047325</td>\n",
       "      <td>-0.065456</td>\n",
       "      <td>-0.027758</td>\n",
       "      <td>-0.018844</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>-0.059728</td>\n",
       "      <td>-0.057004</td>\n",
       "      <td>-0.043818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.020635</td>\n",
       "      <td>0.030479</td>\n",
       "      <td>-0.051886</td>\n",
       "      <td>-0.055125</td>\n",
       "      <td>-0.052440</td>\n",
       "      <td>0.051107</td>\n",
       "      <td>-0.022648</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028083</td>\n",
       "      <td>-0.061142</td>\n",
       "      <td>-0.018120</td>\n",
       "      <td>-0.061137</td>\n",
       "      <td>-0.049363</td>\n",
       "      <td>-0.026236</td>\n",
       "      <td>0.053327</td>\n",
       "      <td>-0.014145</td>\n",
       "      <td>-0.046835</td>\n",
       "      <td>-0.060753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.044237</td>\n",
       "      <td>-0.020313</td>\n",
       "      <td>-0.048073</td>\n",
       "      <td>-0.056209</td>\n",
       "      <td>0.043061</td>\n",
       "      <td>0.056523</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>0.027694</td>\n",
       "      <td>0.040570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058062</td>\n",
       "      <td>-0.058796</td>\n",
       "      <td>-0.045975</td>\n",
       "      <td>-0.058796</td>\n",
       "      <td>0.050993</td>\n",
       "      <td>-0.033669</td>\n",
       "      <td>0.041798</td>\n",
       "      <td>-0.057656</td>\n",
       "      <td>-0.012462</td>\n",
       "      <td>-0.054203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.035588</td>\n",
       "      <td>0.058073</td>\n",
       "      <td>-0.050484</td>\n",
       "      <td>-0.037032</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>0.060505</td>\n",
       "      <td>0.057046</td>\n",
       "      <td>-0.047282</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059808</td>\n",
       "      <td>-0.061199</td>\n",
       "      <td>-0.018811</td>\n",
       "      <td>-0.061199</td>\n",
       "      <td>-0.020907</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>0.028944</td>\n",
       "      <td>-0.047205</td>\n",
       "      <td>-0.041322</td>\n",
       "      <td>-0.023640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.060424</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.058672</td>\n",
       "      <td>-0.038685</td>\n",
       "      <td>0.032710</td>\n",
       "      <td>0.027704</td>\n",
       "      <td>-0.024062</td>\n",
       "      <td>-0.059046</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061882</td>\n",
       "      <td>-0.062241</td>\n",
       "      <td>-0.037259</td>\n",
       "      <td>-0.062241</td>\n",
       "      <td>0.039949</td>\n",
       "      <td>0.027875</td>\n",
       "      <td>0.060479</td>\n",
       "      <td>-0.058036</td>\n",
       "      <td>-0.061445</td>\n",
       "      <td>-0.058604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.049279</td>\n",
       "      <td>-0.059444</td>\n",
       "      <td>-0.058115</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>-0.035380</td>\n",
       "      <td>0.058563</td>\n",
       "      <td>-0.054313</td>\n",
       "      <td>0.057236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059849</td>\n",
       "      <td>-0.062269</td>\n",
       "      <td>-0.055467</td>\n",
       "      <td>-0.062269</td>\n",
       "      <td>0.023908</td>\n",
       "      <td>-0.041097</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>-0.033178</td>\n",
       "      <td>-0.048842</td>\n",
       "      <td>-0.040750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>5</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>-0.057948</td>\n",
       "      <td>-0.051789</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>0.044273</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>-0.016542</td>\n",
       "      <td>0.047858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038026</td>\n",
       "      <td>-0.064721</td>\n",
       "      <td>-0.034635</td>\n",
       "      <td>-0.064720</td>\n",
       "      <td>0.046162</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.034183</td>\n",
       "      <td>-0.047033</td>\n",
       "      <td>-0.006123</td>\n",
       "      <td>-0.060960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1941 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Group         0         1         2         3         4         5  \\\n",
       "0         1  0.024093  0.003332  0.026175 -0.056424  0.066987  0.064344   \n",
       "1         5  0.000738  0.028788  0.008383  0.015660 -0.002792 -0.023342   \n",
       "2         1 -0.029008  0.036773  0.016296 -0.066768  0.026380  0.067563   \n",
       "3         5  0.025092 -0.005877  0.036179 -0.007282  0.044100  0.047946   \n",
       "4         5 -0.020635  0.030479 -0.051886 -0.055125 -0.052440  0.051107   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "1936      4 -0.044237 -0.020313 -0.048073 -0.056209  0.043061  0.056523   \n",
       "1937      5 -0.035588  0.058073 -0.050484 -0.037032  0.016111  0.060505   \n",
       "1938      0 -0.060424 -0.000134 -0.058672 -0.038685  0.032710  0.027704   \n",
       "1939      0 -0.013489  0.049279 -0.059444 -0.058115 -0.059752 -0.035380   \n",
       "1940      5  0.019092  0.012424 -0.057948 -0.051789  0.025150  0.044273   \n",
       "\n",
       "             6         7         8  ...       502       503       504  \\\n",
       "0    -0.015630 -0.065082  0.063505  ... -0.030333 -0.068542  0.004144   \n",
       "1     0.039279 -0.006778 -0.046869  ... -0.064577 -0.065933 -0.046359   \n",
       "2     0.040172 -0.060464 -0.026711  ... -0.009418 -0.068801 -0.007311   \n",
       "3     0.032226  0.004009  0.051182  ... -0.057155 -0.065491 -0.047325   \n",
       "4    -0.022648 -0.009215  0.052870  ... -0.028083 -0.061142 -0.018120   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1936 -0.000632  0.027694  0.040570  ... -0.058062 -0.058796 -0.045975   \n",
       "1937  0.057046 -0.047282  0.031583  ... -0.059808 -0.061199 -0.018811   \n",
       "1938 -0.024062 -0.059046  0.029137  ... -0.061882 -0.062241 -0.037259   \n",
       "1939  0.058563 -0.054313  0.057236  ... -0.059849 -0.062269 -0.055467   \n",
       "1940 -0.001287 -0.016542  0.047858  ... -0.038026 -0.064721 -0.034635   \n",
       "\n",
       "           505       506       507       508       509       510       511  \n",
       "0    -0.068533 -0.047693 -0.045203  0.038434 -0.052579 -0.026946 -0.064774  \n",
       "1    -0.065978 -0.057970 -0.046468  0.047742 -0.053377 -0.034778  0.007275  \n",
       "2    -0.068773 -0.016543  0.005688  0.041948 -0.049344 -0.005887 -0.032376  \n",
       "3    -0.065456 -0.027758 -0.018844  0.033387 -0.059728 -0.057004 -0.043818  \n",
       "4    -0.061137 -0.049363 -0.026236  0.053327 -0.014145 -0.046835 -0.060753  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1936 -0.058796  0.050993 -0.033669  0.041798 -0.057656 -0.012462 -0.054203  \n",
       "1937 -0.061199 -0.020907 -0.026319  0.028944 -0.047205 -0.041322 -0.023640  \n",
       "1938 -0.062241  0.039949  0.027875  0.060479 -0.058036 -0.061445 -0.058604  \n",
       "1939 -0.062269  0.023908 -0.041097  0.012535 -0.033178 -0.048842 -0.040750  \n",
       "1940 -0.064720  0.046162  0.005062  0.034183 -0.047033 -0.006123 -0.060960  \n",
       "\n",
       "[1941 rows x 513 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imputed=df1.drop(['title', 'abstract'], axis=1)\n",
    "df_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to predict the X and y data as follows:\n",
    "\n",
    "if 'Group' in df_imputed:\n",
    "    y = df_imputed['Group'].values # get the labels we want\n",
    "    del df_imputed['Group'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #it's good practice to Scale Data\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled_train = scaler.fit_transform(X)\n",
    "# scaled_train_df = pd.DataFrame(scaled_train, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n",
      "1552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(y_test))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tb = TensorBoard(log_dir=f\"logs\\\\{time()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution() #disable eager execution,since got error AttributeError: Tensor.graph is meaningless when eager execution is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES=512\n",
    "weight_decay=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               153900    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 1806      \n",
      "=================================================================\n",
      "Total params: 426,606\n",
      "Trainable params: 426,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1552 samples, validate on 389 samples\n",
      "Epoch 1/74\n",
      "1552/1552 [==============================] - 1s 435us/sample - loss: 1.4473 - accuracy: 0.4504 - val_loss: 1.0100 - val_accuracy: 0.6272\n",
      "Epoch 2/74\n",
      "1552/1552 [==============================] - 0s 125us/sample - loss: 0.8337 - accuracy: 0.7113 - val_loss: 0.6262 - val_accuracy: 0.7815\n",
      "Epoch 3/74\n",
      "1552/1552 [==============================] - 0s 127us/sample - loss: 0.5977 - accuracy: 0.7906 - val_loss: 0.5110 - val_accuracy: 0.7995\n",
      "Epoch 4/74\n",
      "1552/1552 [==============================] - 0s 129us/sample - loss: 0.5208 - accuracy: 0.8138 - val_loss: 0.5358 - val_accuracy: 0.8098\n",
      "Epoch 5/74\n",
      "1552/1552 [==============================] - 0s 130us/sample - loss: 0.4692 - accuracy: 0.8286 - val_loss: 0.5032 - val_accuracy: 0.8046\n",
      "Epoch 6/74\n",
      "1552/1552 [==============================] - 0s 128us/sample - loss: 0.3920 - accuracy: 0.8615 - val_loss: 0.4641 - val_accuracy: 0.8252\n",
      "Epoch 7/74\n",
      "1552/1552 [==============================] - 0s 136us/sample - loss: 0.3110 - accuracy: 0.8950 - val_loss: 0.4221 - val_accuracy: 0.8406\n",
      "Epoch 8/74\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 0.2829 - accuracy: 0.9014 - val_loss: 0.4124 - val_accuracy: 0.8560\n",
      "Epoch 9/74\n",
      "1552/1552 [==============================] - 0s 134us/sample - loss: 0.2600 - accuracy: 0.9085 - val_loss: 0.4968 - val_accuracy: 0.8355\n",
      "Epoch 10/74\n",
      "1552/1552 [==============================] - 0s 124us/sample - loss: 0.2472 - accuracy: 0.9137 - val_loss: 0.4861 - val_accuracy: 0.8406\n",
      "Epoch 11/74\n",
      "1552/1552 [==============================] - 0s 126us/sample - loss: 0.1813 - accuracy: 0.9375 - val_loss: 0.4756 - val_accuracy: 0.8483\n",
      "Epoch 12/74\n",
      "1552/1552 [==============================] - 0s 125us/sample - loss: 0.1181 - accuracy: 0.9684 - val_loss: 0.5116 - val_accuracy: 0.8458\n",
      "Epoch 13/74\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 0.0949 - accuracy: 0.9716 - val_loss: 0.5286 - val_accuracy: 0.8278\n",
      "Epoch 14/74\n",
      "1552/1552 [==============================] - 0s 118us/sample - loss: 0.0891 - accuracy: 0.9716 - val_loss: 0.5228 - val_accuracy: 0.8355\n",
      "Epoch 15/74\n",
      "1552/1552 [==============================] - 0s 139us/sample - loss: 0.0798 - accuracy: 0.9723 - val_loss: 0.5844 - val_accuracy: 0.8380\n",
      "Epoch 16/74\n",
      "1552/1552 [==============================] - 0s 152us/sample - loss: 0.0619 - accuracy: 0.9794 - val_loss: 0.5274 - val_accuracy: 0.8509\n",
      "Epoch 17/74\n",
      "1552/1552 [==============================] - 0s 162us/sample - loss: 0.0503 - accuracy: 0.9839 - val_loss: 0.7381 - val_accuracy: 0.8329\n",
      "Epoch 18/74\n",
      "1552/1552 [==============================] - 0s 161us/sample - loss: 0.0500 - accuracy: 0.9858 - val_loss: 0.6272 - val_accuracy: 0.8380\n",
      "Epoch 19/74\n",
      "1552/1552 [==============================] - 0s 157us/sample - loss: 0.0283 - accuracy: 0.9936 - val_loss: 0.6716 - val_accuracy: 0.8252\n",
      "Epoch 20/74\n",
      "1552/1552 [==============================] - 0s 172us/sample - loss: 0.0211 - accuracy: 0.9955 - val_loss: 0.6490 - val_accuracy: 0.8483\n",
      "Epoch 21/74\n",
      "1552/1552 [==============================] - 0s 237us/sample - loss: 0.0149 - accuracy: 0.9994 - val_loss: 0.6700 - val_accuracy: 0.8458\n",
      "Epoch 22/74\n",
      "1552/1552 [==============================] - 0s 260us/sample - loss: 0.0162 - accuracy: 0.9981 - val_loss: 0.7309 - val_accuracy: 0.8252\n",
      "Epoch 23/74\n",
      "1552/1552 [==============================] - 0s 255us/sample - loss: 0.0231 - accuracy: 0.9942 - val_loss: 0.8751 - val_accuracy: 0.8123\n",
      "Epoch 24/74\n",
      "1552/1552 [==============================] - 0s 245us/sample - loss: 0.0112 - accuracy: 0.9981 - val_loss: 0.7518 - val_accuracy: 0.8432\n",
      "Epoch 25/74\n",
      "1552/1552 [==============================] - 0s 281us/sample - loss: 0.0114 - accuracy: 0.9994 - val_loss: 0.7537 - val_accuracy: 0.8355\n",
      "Epoch 26/74\n",
      "1552/1552 [==============================] - 0s 197us/sample - loss: 0.0128 - accuracy: 0.9981 - val_loss: 0.7627 - val_accuracy: 0.8329\n",
      "Epoch 27/74\n",
      "1552/1552 [==============================] - 0s 163us/sample - loss: 0.0213 - accuracy: 0.9955 - val_loss: 0.7834 - val_accuracy: 0.8355\n",
      "Epoch 28/74\n",
      "1552/1552 [==============================] - 0s 168us/sample - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.7613 - val_accuracy: 0.8380\n",
      "Epoch 29/74\n",
      "1552/1552 [==============================] - 0s 220us/sample - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.8596 - val_accuracy: 0.8149\n",
      "Epoch 30/74\n",
      "1552/1552 [==============================] - 0s 221us/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.8728 - val_accuracy: 0.8226\n",
      "Epoch 31/74\n",
      "1552/1552 [==============================] - 0s 193us/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.8520 - val_accuracy: 0.8303\n",
      "Epoch 32/74\n",
      "1552/1552 [==============================] - 0s 227us/sample - loss: 0.0098 - accuracy: 0.9987 - val_loss: 0.9390 - val_accuracy: 0.8175\n",
      "Epoch 33/74\n",
      "1552/1552 [==============================] - 0s 269us/sample - loss: 0.0081 - accuracy: 0.9994 - val_loss: 0.9870 - val_accuracy: 0.8406\n",
      "Epoch 34/74\n",
      "1552/1552 [==============================] - 0s 250us/sample - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.8970 - val_accuracy: 0.8303\n",
      "Epoch 35/74\n",
      "1552/1552 [==============================] - 0s 220us/sample - loss: 0.0095 - accuracy: 0.9987 - val_loss: 0.8981 - val_accuracy: 0.8355\n",
      "Epoch 36/74\n",
      "1552/1552 [==============================] - 0s 164us/sample - loss: 0.0119 - accuracy: 0.9987 - val_loss: 0.8968 - val_accuracy: 0.8278\n",
      "Epoch 37/74\n",
      "1552/1552 [==============================] - 0s 163us/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.9165 - val_accuracy: 0.8406\n",
      "Epoch 38/74\n",
      "1552/1552 [==============================] - 0s 170us/sample - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.9755 - val_accuracy: 0.8021\n",
      "Epoch 39/74\n",
      "1552/1552 [==============================] - 0s 193us/sample - loss: 0.0117 - accuracy: 0.9981 - val_loss: 1.2674 - val_accuracy: 0.8098\n",
      "Epoch 40/74\n",
      "1552/1552 [==============================] - 0s 157us/sample - loss: 0.0203 - accuracy: 0.9929 - val_loss: 1.0895 - val_accuracy: 0.8278\n",
      "Epoch 41/74\n",
      "1552/1552 [==============================] - 0s 157us/sample - loss: 0.1397 - accuracy: 0.9613 - val_loss: 0.7863 - val_accuracy: 0.8175\n",
      "Epoch 42/74\n",
      "1552/1552 [==============================] - 0s 207us/sample - loss: 0.0602 - accuracy: 0.9813 - val_loss: 0.7482 - val_accuracy: 0.8149\n",
      "Epoch 43/74\n",
      "1552/1552 [==============================] - 0s 191us/sample - loss: 0.0467 - accuracy: 0.9878 - val_loss: 0.8181 - val_accuracy: 0.8046\n",
      "Epoch 44/74\n",
      "1552/1552 [==============================] - 0s 141us/sample - loss: 0.0268 - accuracy: 0.9936 - val_loss: 0.7944 - val_accuracy: 0.8329\n",
      "Epoch 45/74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552/1552 [==============================] - 0s 186us/sample - loss: 0.0202 - accuracy: 0.9942 - val_loss: 0.7590 - val_accuracy: 0.8252\n",
      "Epoch 46/74\n",
      "1552/1552 [==============================] - 0s 137us/sample - loss: 0.0095 - accuracy: 0.9987 - val_loss: 0.8775 - val_accuracy: 0.8406\n",
      "Epoch 47/74\n",
      "1552/1552 [==============================] - 0s 188us/sample - loss: 0.0099 - accuracy: 0.9987 - val_loss: 0.8753 - val_accuracy: 0.8303\n",
      "Epoch 48/74\n",
      "1552/1552 [==============================] - 0s 161us/sample - loss: 0.0098 - accuracy: 0.9987 - val_loss: 0.9138 - val_accuracy: 0.8406\n",
      "Epoch 49/74\n",
      "1552/1552 [==============================] - 0s 165us/sample - loss: 0.0130 - accuracy: 0.9974 - val_loss: 0.8345 - val_accuracy: 0.8406\n",
      "Epoch 50/74\n",
      "1552/1552 [==============================] - 0s 145us/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.8707 - val_accuracy: 0.8226\n",
      "Epoch 51/74\n",
      "1552/1552 [==============================] - 0s 138us/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.8632 - val_accuracy: 0.8303\n",
      "Epoch 52/74\n",
      "1552/1552 [==============================] - 0s 148us/sample - loss: 0.0072 - accuracy: 0.9994 - val_loss: 0.9041 - val_accuracy: 0.8278\n",
      "Epoch 53/74\n",
      "1552/1552 [==============================] - 0s 178us/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.8940 - val_accuracy: 0.8303\n",
      "Epoch 54/74\n",
      "1552/1552 [==============================] - 0s 196us/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.9093 - val_accuracy: 0.8252\n",
      "Epoch 55/74\n",
      "1552/1552 [==============================] - 0s 205us/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.9198 - val_accuracy: 0.8278\n",
      "Epoch 56/74\n",
      "1552/1552 [==============================] - 0s 158us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.9175 - val_accuracy: 0.8329\n",
      "Epoch 57/74\n",
      "1552/1552 [==============================] - 0s 158us/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.9281 - val_accuracy: 0.8303\n",
      "Epoch 58/74\n",
      "1552/1552 [==============================] - 0s 213us/sample - loss: 0.0076 - accuracy: 0.9994 - val_loss: 0.9060 - val_accuracy: 0.8278\n",
      "Epoch 59/74\n",
      "1552/1552 [==============================] - 0s 218us/sample - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.9859 - val_accuracy: 0.8175\n",
      "Epoch 60/74\n",
      "1552/1552 [==============================] - 0s 229us/sample - loss: 0.0095 - accuracy: 0.9981 - val_loss: 0.9984 - val_accuracy: 0.8303\n",
      "Epoch 61/74\n",
      "1552/1552 [==============================] - 0s 212us/sample - loss: 0.0084 - accuracy: 0.9987 - val_loss: 0.9154 - val_accuracy: 0.8380\n",
      "Epoch 62/74\n",
      "1552/1552 [==============================] - 0s 178us/sample - loss: 0.0109 - accuracy: 0.9974 - val_loss: 1.0229 - val_accuracy: 0.8380\n",
      "Epoch 63/74\n",
      "1552/1552 [==============================] - 0s 183us/sample - loss: 0.0157 - accuracy: 0.9961 - val_loss: 1.0182 - val_accuracy: 0.8278\n",
      "Epoch 64/74\n",
      "1552/1552 [==============================] - 0s 213us/sample - loss: 0.0291 - accuracy: 0.9916 - val_loss: 0.9220 - val_accuracy: 0.8278\n",
      "Epoch 65/74\n",
      "1552/1552 [==============================] - 0s 237us/sample - loss: 0.0170 - accuracy: 0.9955 - val_loss: 0.8839 - val_accuracy: 0.8303\n",
      "Epoch 66/74\n",
      "1552/1552 [==============================] - 0s 162us/sample - loss: 0.0149 - accuracy: 0.9968 - val_loss: 0.9171 - val_accuracy: 0.8149\n",
      "Epoch 67/74\n",
      "1552/1552 [==============================] - 0s 186us/sample - loss: 0.0079 - accuracy: 0.9994 - val_loss: 0.9648 - val_accuracy: 0.8252\n",
      "Epoch 68/74\n",
      "1552/1552 [==============================] - 0s 171us/sample - loss: 0.0107 - accuracy: 0.9981 - val_loss: 1.0197 - val_accuracy: 0.8252\n",
      "Epoch 69/74\n",
      "1552/1552 [==============================] - 0s 180us/sample - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.9680 - val_accuracy: 0.8252\n",
      "Epoch 70/74\n",
      "1552/1552 [==============================] - 0s 214us/sample - loss: 0.0077 - accuracy: 0.9987 - val_loss: 0.9806 - val_accuracy: 0.8432\n",
      "Epoch 71/74\n",
      "1552/1552 [==============================] - 0s 195us/sample - loss: 0.0078 - accuracy: 0.9994 - val_loss: 1.0049 - val_accuracy: 0.8278\n",
      "Epoch 72/74\n",
      "1552/1552 [==============================] - 0s 172us/sample - loss: 0.0073 - accuracy: 0.9987 - val_loss: 1.0319 - val_accuracy: 0.8252\n",
      "Epoch 73/74\n",
      "1552/1552 [==============================] - 0s 181us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.9768 - val_accuracy: 0.8278\n",
      "Epoch 74/74\n",
      "1552/1552 [==============================] - 0s 209us/sample - loss: 0.0078 - accuracy: 0.9994 - val_loss: 1.0815 - val_accuracy: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bd604437f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='relu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                 #kernel_regularizer=regularizers.l2(0.001),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dropout(0.1),\n",
    "        layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    #layers.Dense(units, activation=activation_func),\n",
    "    #layers.SimpleRNN(100,unroll=True),\n",
    "    layers.Dense(6 ,activation='softmax')\n",
    "])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='sparse_categorical_crossentropy', patience=10)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=74, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               153900    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 1806      \n",
      "=================================================================\n",
      "Total params: 426,606\n",
      "Trainable params: 426,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1552 samples, validate on 389 samples\n",
      "Epoch 1/150\n",
      "1552/1552 [==============================] - 0s 267us/sample - loss: 1.7748 - accuracy: 0.2829 - val_loss: 1.7059 - val_accuracy: 0.3470\n",
      "Epoch 2/150\n",
      "1552/1552 [==============================] - 0s 85us/sample - loss: 1.6801 - accuracy: 0.3666 - val_loss: 1.5668 - val_accuracy: 0.3393\n",
      "Epoch 3/150\n",
      "1552/1552 [==============================] - 0s 102us/sample - loss: 1.5414 - accuracy: 0.3531 - val_loss: 1.4402 - val_accuracy: 0.3393\n",
      "Epoch 4/150\n",
      "1552/1552 [==============================] - 0s 137us/sample - loss: 1.4153 - accuracy: 0.3666 - val_loss: 1.3041 - val_accuracy: 0.6812\n",
      "Epoch 5/150\n",
      "1552/1552 [==============================] - 0s 100us/sample - loss: 1.2836 - accuracy: 0.6314 - val_loss: 1.1754 - val_accuracy: 0.6041\n",
      "Epoch 6/150\n",
      "1552/1552 [==============================] - 0s 107us/sample - loss: 1.1554 - accuracy: 0.6186 - val_loss: 1.0267 - val_accuracy: 0.6607\n",
      "Epoch 7/150\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 1.0098 - accuracy: 0.6849 - val_loss: 0.9268 - val_accuracy: 0.6838\n",
      "Epoch 8/150\n",
      "1552/1552 [==============================] - 0s 152us/sample - loss: 0.9190 - accuracy: 0.6927 - val_loss: 0.8398 - val_accuracy: 0.6812\n",
      "Epoch 9/150\n",
      "1552/1552 [==============================] - 0s 121us/sample - loss: 0.8419 - accuracy: 0.7030 - val_loss: 0.7648 - val_accuracy: 0.6967\n",
      "Epoch 10/150\n",
      "1552/1552 [==============================] - 0s 108us/sample - loss: 0.7676 - accuracy: 0.7113 - val_loss: 0.7029 - val_accuracy: 0.7224\n",
      "Epoch 11/150\n",
      "1552/1552 [==============================] - 0s 116us/sample - loss: 0.7063 - accuracy: 0.7571 - val_loss: 0.6436 - val_accuracy: 0.7841\n",
      "Epoch 12/150\n",
      "1552/1552 [==============================] - 0s 128us/sample - loss: 0.6480 - accuracy: 0.7874 - val_loss: 0.5906 - val_accuracy: 0.7918\n",
      "Epoch 13/150\n",
      "1552/1552 [==============================] - 0s 147us/sample - loss: 0.6004 - accuracy: 0.8022 - val_loss: 0.5488 - val_accuracy: 0.7995\n",
      "Epoch 14/150\n",
      "1552/1552 [==============================] - 0s 132us/sample - loss: 0.5638 - accuracy: 0.8048 - val_loss: 0.5241 - val_accuracy: 0.8021\n",
      "Epoch 15/150\n",
      "1552/1552 [==============================] - 0s 104us/sample - loss: 0.5241 - accuracy: 0.8138 - val_loss: 0.5148 - val_accuracy: 0.8149\n",
      "Epoch 16/150\n",
      "1552/1552 [==============================] - 0s 112us/sample - loss: 0.5120 - accuracy: 0.8157 - val_loss: 0.4948 - val_accuracy: 0.8123\n",
      "Epoch 17/150\n",
      "1552/1552 [==============================] - 0s 112us/sample - loss: 0.4893 - accuracy: 0.8254 - val_loss: 0.4874 - val_accuracy: 0.8278\n",
      "Epoch 18/150\n",
      "1552/1552 [==============================] - 0s 94us/sample - loss: 0.4772 - accuracy: 0.8305 - val_loss: 0.4721 - val_accuracy: 0.8226\n",
      "Epoch 19/150\n",
      "1552/1552 [==============================] - 0s 123us/sample - loss: 0.4743 - accuracy: 0.8267 - val_loss: 0.4678 - val_accuracy: 0.8175\n",
      "Epoch 20/150\n",
      "1552/1552 [==============================] - 0s 112us/sample - loss: 0.4406 - accuracy: 0.8389 - val_loss: 0.4666 - val_accuracy: 0.8149\n",
      "Epoch 21/150\n",
      "1552/1552 [==============================] - 0s 122us/sample - loss: 0.4247 - accuracy: 0.8421 - val_loss: 0.4498 - val_accuracy: 0.8175\n",
      "Epoch 22/150\n",
      "1552/1552 [==============================] - 0s 103us/sample - loss: 0.4021 - accuracy: 0.8544 - val_loss: 0.4699 - val_accuracy: 0.8201\n",
      "Epoch 23/150\n",
      "1552/1552 [==============================] - 0s 120us/sample - loss: 0.3963 - accuracy: 0.8518 - val_loss: 0.4527 - val_accuracy: 0.8252\n",
      "Epoch 24/150\n",
      "1552/1552 [==============================] - 0s 98us/sample - loss: 0.3773 - accuracy: 0.8634 - val_loss: 0.4417 - val_accuracy: 0.8278\n",
      "Epoch 25/150\n",
      "1552/1552 [==============================] - 0s 121us/sample - loss: 0.3480 - accuracy: 0.8776 - val_loss: 0.4554 - val_accuracy: 0.8278\n",
      "Epoch 26/150\n",
      "1552/1552 [==============================] - 0s 102us/sample - loss: 0.3382 - accuracy: 0.8866 - val_loss: 0.4447 - val_accuracy: 0.8355\n",
      "Epoch 27/150\n",
      "1552/1552 [==============================] - 0s 113us/sample - loss: 0.3191 - accuracy: 0.8956 - val_loss: 0.4559 - val_accuracy: 0.8303\n",
      "Epoch 28/150\n",
      "1552/1552 [==============================] - 0s 102us/sample - loss: 0.2971 - accuracy: 0.8982 - val_loss: 0.4383 - val_accuracy: 0.8278\n",
      "Epoch 29/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.2694 - accuracy: 0.9156 - val_loss: 0.4363 - val_accuracy: 0.8432\n",
      "Epoch 30/150\n",
      "1552/1552 [==============================] - 0s 112us/sample - loss: 0.2482 - accuracy: 0.9149 - val_loss: 0.4602 - val_accuracy: 0.8303\n",
      "Epoch 31/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.2396 - accuracy: 0.9214 - val_loss: 0.4285 - val_accuracy: 0.8458\n",
      "Epoch 32/150\n",
      "1552/1552 [==============================] - 0s 121us/sample - loss: 0.2169 - accuracy: 0.9323 - val_loss: 0.4536 - val_accuracy: 0.8432\n",
      "Epoch 33/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.2159 - accuracy: 0.9227 - val_loss: 0.4332 - val_accuracy: 0.8432\n",
      "Epoch 34/150\n",
      "1552/1552 [==============================] - 0s 102us/sample - loss: 0.2010 - accuracy: 0.9330 - val_loss: 0.4484 - val_accuracy: 0.8483\n",
      "Epoch 35/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.1742 - accuracy: 0.9420 - val_loss: 0.4510 - val_accuracy: 0.8560\n",
      "Epoch 36/150\n",
      "1552/1552 [==============================] - 0s 103us/sample - loss: 0.1694 - accuracy: 0.9439 - val_loss: 0.4386 - val_accuracy: 0.8535\n",
      "Epoch 37/150\n",
      "1552/1552 [==============================] - 0s 111us/sample - loss: 0.1530 - accuracy: 0.9536 - val_loss: 0.4556 - val_accuracy: 0.8586\n",
      "Epoch 38/150\n",
      "1552/1552 [==============================] - 0s 108us/sample - loss: 0.1478 - accuracy: 0.9607 - val_loss: 0.4709 - val_accuracy: 0.8560\n",
      "Epoch 39/150\n",
      "1552/1552 [==============================] - 0s 107us/sample - loss: 0.1335 - accuracy: 0.9613 - val_loss: 0.4545 - val_accuracy: 0.8535\n",
      "Epoch 40/150\n",
      "1552/1552 [==============================] - 0s 117us/sample - loss: 0.1185 - accuracy: 0.9652 - val_loss: 0.4747 - val_accuracy: 0.8535\n",
      "Epoch 41/150\n",
      "1552/1552 [==============================] - 0s 102us/sample - loss: 0.1154 - accuracy: 0.9684 - val_loss: 0.4847 - val_accuracy: 0.8509\n",
      "Epoch 42/150\n",
      "1552/1552 [==============================] - 0s 111us/sample - loss: 0.1023 - accuracy: 0.9710 - val_loss: 0.4986 - val_accuracy: 0.8509\n",
      "Epoch 43/150\n",
      "1552/1552 [==============================] - 0s 107us/sample - loss: 0.0963 - accuracy: 0.9736 - val_loss: 0.5154 - val_accuracy: 0.8458\n",
      "Epoch 44/150\n",
      "1552/1552 [==============================] - 0s 116us/sample - loss: 0.0910 - accuracy: 0.9762 - val_loss: 0.5183 - val_accuracy: 0.8535\n",
      "Epoch 45/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552/1552 [==============================] - 0s 119us/sample - loss: 0.0700 - accuracy: 0.9865 - val_loss: 0.5517 - val_accuracy: 0.8406\n",
      "Epoch 46/150\n",
      "1552/1552 [==============================] - 0s 118us/sample - loss: 0.0739 - accuracy: 0.9832 - val_loss: 0.5525 - val_accuracy: 0.8432\n",
      "Epoch 47/150\n",
      "1552/1552 [==============================] - 0s 99us/sample - loss: 0.0616 - accuracy: 0.9858 - val_loss: 0.5419 - val_accuracy: 0.8406\n",
      "Epoch 48/150\n",
      "1552/1552 [==============================] - 0s 117us/sample - loss: 0.0583 - accuracy: 0.9858 - val_loss: 0.5726 - val_accuracy: 0.8509\n",
      "Epoch 49/150\n",
      "1552/1552 [==============================] - 0s 105us/sample - loss: 0.0490 - accuracy: 0.9903 - val_loss: 0.5906 - val_accuracy: 0.8535\n",
      "Epoch 50/150\n",
      "1552/1552 [==============================] - 0s 118us/sample - loss: 0.0466 - accuracy: 0.9916 - val_loss: 0.5795 - val_accuracy: 0.8432\n",
      "Epoch 51/150\n",
      "1552/1552 [==============================] - 0s 110us/sample - loss: 0.0420 - accuracy: 0.9936 - val_loss: 0.6010 - val_accuracy: 0.8483\n",
      "Epoch 52/150\n",
      "1552/1552 [==============================] - 0s 129us/sample - loss: 0.0379 - accuracy: 0.9942 - val_loss: 0.6294 - val_accuracy: 0.8458\n",
      "Epoch 53/150\n",
      "1552/1552 [==============================] - 0s 116us/sample - loss: 0.0350 - accuracy: 0.9955 - val_loss: 0.6370 - val_accuracy: 0.8355\n",
      "Epoch 54/150\n",
      "1552/1552 [==============================] - 0s 105us/sample - loss: 0.0320 - accuracy: 0.9955 - val_loss: 0.6534 - val_accuracy: 0.8406\n",
      "Epoch 55/150\n",
      "1552/1552 [==============================] - 0s 123us/sample - loss: 0.0280 - accuracy: 0.9955 - val_loss: 0.6623 - val_accuracy: 0.8432\n",
      "Epoch 56/150\n",
      "1552/1552 [==============================] - 0s 103us/sample - loss: 0.0298 - accuracy: 0.9955 - val_loss: 0.6417 - val_accuracy: 0.8278\n",
      "Epoch 57/150\n",
      "1552/1552 [==============================] - 0s 161us/sample - loss: 0.0236 - accuracy: 0.9987 - val_loss: 0.6698 - val_accuracy: 0.8406\n",
      "Epoch 58/150\n",
      "1552/1552 [==============================] - 0s 175us/sample - loss: 0.0220 - accuracy: 0.9968 - val_loss: 0.6914 - val_accuracy: 0.8329\n",
      "Epoch 59/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.0207 - accuracy: 0.9974 - val_loss: 0.6628 - val_accuracy: 0.8329\n",
      "Epoch 60/150\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 0.0197 - accuracy: 0.9987 - val_loss: 0.6699 - val_accuracy: 0.8303\n",
      "Epoch 61/150\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 0.0182 - accuracy: 0.9987 - val_loss: 0.7041 - val_accuracy: 0.8406\n",
      "Epoch 62/150\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 0.0168 - accuracy: 0.9994 - val_loss: 0.7210 - val_accuracy: 0.8406\n",
      "Epoch 63/150\n",
      "1552/1552 [==============================] - 0s 123us/sample - loss: 0.0158 - accuracy: 0.9987 - val_loss: 0.7177 - val_accuracy: 0.8355\n",
      "Epoch 64/150\n",
      "1552/1552 [==============================] - 0s 113us/sample - loss: 0.0132 - accuracy: 0.9987 - val_loss: 0.7320 - val_accuracy: 0.8380\n",
      "Epoch 65/150\n",
      "1552/1552 [==============================] - 0s 108us/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.7335 - val_accuracy: 0.8303\n",
      "Epoch 66/150\n",
      "1552/1552 [==============================] - 0s 157us/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.7584 - val_accuracy: 0.8355\n",
      "Epoch 67/150\n",
      "1552/1552 [==============================] - 0s 170us/sample - loss: 0.0143 - accuracy: 0.9981 - val_loss: 0.7735 - val_accuracy: 0.8303\n",
      "Epoch 68/150\n",
      "1552/1552 [==============================] - 0s 144us/sample - loss: 0.0116 - accuracy: 0.9987 - val_loss: 0.7768 - val_accuracy: 0.8329\n",
      "Epoch 69/150\n",
      "1552/1552 [==============================] - 0s 127us/sample - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.7757 - val_accuracy: 0.8406\n",
      "Epoch 70/150\n",
      "1552/1552 [==============================] - 0s 182us/sample - loss: 0.0125 - accuracy: 0.9981 - val_loss: 0.7775 - val_accuracy: 0.8380\n",
      "Epoch 71/150\n",
      "1552/1552 [==============================] - 0s 130us/sample - loss: 0.0102 - accuracy: 0.9994 - val_loss: 0.7887 - val_accuracy: 0.8406\n",
      "Epoch 72/150\n",
      "1552/1552 [==============================] - 0s 125us/sample - loss: 0.0119 - accuracy: 0.9987 - val_loss: 0.8121 - val_accuracy: 0.8355\n",
      "Epoch 73/150\n",
      "1552/1552 [==============================] - 0s 130us/sample - loss: 0.0104 - accuracy: 0.9987 - val_loss: 0.8018 - val_accuracy: 0.8278\n",
      "Epoch 74/150\n",
      "1552/1552 [==============================] - 0s 165us/sample - loss: 0.0099 - accuracy: 0.9994 - val_loss: 0.7856 - val_accuracy: 0.8458\n",
      "Epoch 75/150\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 0.0102 - accuracy: 0.9994 - val_loss: 0.8202 - val_accuracy: 0.8380\n",
      "Epoch 76/150\n",
      "1552/1552 [==============================] - 0s 105us/sample - loss: 0.0098 - accuracy: 0.9994 - val_loss: 0.8526 - val_accuracy: 0.8355\n",
      "Epoch 77/150\n",
      "1552/1552 [==============================] - 0s 125us/sample - loss: 0.0093 - accuracy: 0.9994 - val_loss: 0.8375 - val_accuracy: 0.8252\n",
      "Epoch 78/150\n",
      "1552/1552 [==============================] - 0s 111us/sample - loss: 0.0095 - accuracy: 0.9994 - val_loss: 0.8466 - val_accuracy: 0.8226\n",
      "Epoch 79/150\n",
      "1552/1552 [==============================] - 0s 132us/sample - loss: 0.0083 - accuracy: 0.9994 - val_loss: 0.8802 - val_accuracy: 0.8329\n",
      "Epoch 80/150\n",
      "1552/1552 [==============================] - 0s 119us/sample - loss: 0.0092 - accuracy: 0.9994 - val_loss: 0.8871 - val_accuracy: 0.8329\n",
      "Epoch 81/150\n",
      "1552/1552 [==============================] - 0s 107us/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.8726 - val_accuracy: 0.8303\n",
      "Epoch 82/150\n",
      "1552/1552 [==============================] - 0s 116us/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.8721 - val_accuracy: 0.8303\n",
      "Epoch 83/150\n",
      "1552/1552 [==============================] - 0s 113us/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.8807 - val_accuracy: 0.8380\n",
      "Epoch 84/150\n",
      "1552/1552 [==============================] - 0s 111us/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.8990 - val_accuracy: 0.8303\n",
      "Epoch 85/150\n",
      "1552/1552 [==============================] - 0s 194us/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.9022 - val_accuracy: 0.8278\n",
      "Epoch 86/150\n",
      "1552/1552 [==============================] - 0s 145us/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.8858 - val_accuracy: 0.8432\n",
      "Epoch 87/150\n",
      "1552/1552 [==============================] - 0s 143us/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.8905 - val_accuracy: 0.8355\n",
      "Epoch 88/150\n",
      "1552/1552 [==============================] - 0s 151us/sample - loss: 0.0070 - accuracy: 0.9994 - val_loss: 0.8871 - val_accuracy: 0.8380\n",
      "Epoch 89/150\n",
      "1552/1552 [==============================] - 0s 168us/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.8878 - val_accuracy: 0.8458\n",
      "Epoch 90/150\n",
      "1552/1552 [==============================] - 0s 140us/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.9110 - val_accuracy: 0.8355\n",
      "Epoch 91/150\n",
      "1552/1552 [==============================] - 0s 147us/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.9253 - val_accuracy: 0.8329\n",
      "Epoch 92/150\n",
      "1552/1552 [==============================] - 0s 149us/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.9111 - val_accuracy: 0.8355\n",
      "Epoch 93/150\n",
      "1552/1552 [==============================] - 0s 167us/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.9048 - val_accuracy: 0.8380\n",
      "Epoch 94/150\n",
      "1552/1552 [==============================] - 0s 149us/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.9232 - val_accuracy: 0.8355\n",
      "Epoch 95/150\n",
      "1552/1552 [==============================] - 0s 123us/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.9212 - val_accuracy: 0.8329\n",
      "Epoch 96/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.9039 - val_accuracy: 0.8355\n",
      "Epoch 97/150\n",
      "1552/1552 [==============================] - 0s 120us/sample - loss: 0.0071 - accuracy: 0.9994 - val_loss: 0.9001 - val_accuracy: 0.8380\n",
      "Epoch 98/150\n",
      "1552/1552 [==============================] - 0s 141us/sample - loss: 0.0067 - accuracy: 0.9994 - val_loss: 0.9178 - val_accuracy: 0.8355\n",
      "Epoch 99/150\n",
      "1552/1552 [==============================] - 0s 133us/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.9243 - val_accuracy: 0.8380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "1552/1552 [==============================] - 0s 131us/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.9260 - val_accuracy: 0.8278\n",
      "Epoch 101/150\n",
      "1552/1552 [==============================] - 0s 111us/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.9181 - val_accuracy: 0.8406\n",
      "Epoch 102/150\n",
      "1552/1552 [==============================] - 0s 127us/sample - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.9032 - val_accuracy: 0.8303\n",
      "Epoch 103/150\n",
      "1552/1552 [==============================] - 0s 119us/sample - loss: 0.0070 - accuracy: 0.9994 - val_loss: 0.9184 - val_accuracy: 0.8355\n",
      "Epoch 104/150\n",
      "1552/1552 [==============================] - 0s 106us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.9177 - val_accuracy: 0.8329\n",
      "Epoch 105/150\n",
      "1552/1552 [==============================] - 0s 179us/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.9167 - val_accuracy: 0.8329\n",
      "Epoch 106/150\n",
      "1552/1552 [==============================] - 0s 152us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.9191 - val_accuracy: 0.8355\n",
      "Epoch 107/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.9236 - val_accuracy: 0.8380\n",
      "Epoch 108/150\n",
      "1552/1552 [==============================] - 0s 160us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.9280 - val_accuracy: 0.8355\n",
      "Epoch 109/150\n",
      "1552/1552 [==============================] - 0s 129us/sample - loss: 0.0076 - accuracy: 0.9994 - val_loss: 0.9078 - val_accuracy: 0.8406\n",
      "Epoch 110/150\n",
      "1552/1552 [==============================] - 0s 110us/sample - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.9598 - val_accuracy: 0.8406\n",
      "Epoch 111/150\n",
      "1552/1552 [==============================] - 0s 115us/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.0263 - val_accuracy: 0.8406\n",
      "Epoch 112/150\n",
      "1552/1552 [==============================] - 0s 118us/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.9563 - val_accuracy: 0.8329\n",
      "Epoch 113/150\n",
      "1552/1552 [==============================] - 0s 109us/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.9206 - val_accuracy: 0.8406\n",
      "Epoch 114/150\n",
      "1552/1552 [==============================] - 0s 104us/sample - loss: 0.0067 - accuracy: 0.9994 - val_loss: 0.9228 - val_accuracy: 0.8432\n",
      "Epoch 115/150\n",
      "1552/1552 [==============================] - 0s 117us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.9623 - val_accuracy: 0.8329\n",
      "Epoch 116/150\n",
      "1552/1552 [==============================] - 0s 101us/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.9870 - val_accuracy: 0.8329\n",
      "Epoch 117/150\n",
      "1552/1552 [==============================] - 0s 119us/sample - loss: 0.0067 - accuracy: 0.9994 - val_loss: 0.9791 - val_accuracy: 0.8355\n",
      "Epoch 118/150\n",
      "1552/1552 [==============================] - 0s 105us/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.9480 - val_accuracy: 0.8329\n",
      "Epoch 119/150\n",
      "1552/1552 [==============================] - 0s 104us/sample - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.9402 - val_accuracy: 0.8252\n",
      "Epoch 120/150\n",
      "1552/1552 [==============================] - 0s 107us/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.9550 - val_accuracy: 0.8278\n",
      "Epoch 121/150\n",
      "1552/1552 [==============================] - 0s 114us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.9827 - val_accuracy: 0.8303\n",
      "Epoch 122/150\n",
      "1552/1552 [==============================] - 0s 98us/sample - loss: 0.0063 - accuracy: 0.9994 - val_loss: 0.9830 - val_accuracy: 0.8329\n",
      "Epoch 123/150\n",
      "1552/1552 [==============================] - 0s 110us/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.9908 - val_accuracy: 0.8380\n",
      "Epoch 124/150\n",
      "1552/1552 [==============================] - 0s 116us/sample - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.9658 - val_accuracy: 0.8355\n",
      "Epoch 125/150\n",
      "1552/1552 [==============================] - 0s 105us/sample - loss: 0.0067 - accuracy: 0.9994 - val_loss: 0.9428 - val_accuracy: 0.8303\n",
      "Epoch 126/150\n",
      "1552/1552 [==============================] - 0s 124us/sample - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.9550 - val_accuracy: 0.8329\n",
      "Epoch 127/150\n",
      "1552/1552 [==============================] - 0s 113us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.9857 - val_accuracy: 0.8303\n",
      "Epoch 128/150\n",
      "1552/1552 [==============================] - 0s 132us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.0040 - val_accuracy: 0.8278\n",
      "Epoch 129/150\n",
      "1552/1552 [==============================] - 0s 116us/sample - loss: 0.0088 - accuracy: 0.9994 - val_loss: 0.9468 - val_accuracy: 0.8278\n",
      "Epoch 130/150\n",
      "1552/1552 [==============================] - 0s 123us/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.9226 - val_accuracy: 0.8406\n",
      "Epoch 131/150\n",
      "1552/1552 [==============================] - 0s 122us/sample - loss: 0.0075 - accuracy: 0.9987 - val_loss: 0.9225 - val_accuracy: 0.8432\n",
      "Epoch 132/150\n",
      "1552/1552 [==============================] - 0s 107us/sample - loss: 0.0063 - accuracy: 0.9994 - val_loss: 0.9289 - val_accuracy: 0.8380\n",
      "Epoch 133/150\n",
      "1552/1552 [==============================] - 0s 179us/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.9436 - val_accuracy: 0.8380\n",
      "Epoch 134/150\n",
      "1552/1552 [==============================] - 0s 184us/sample - loss: 0.0061 - accuracy: 0.9994 - val_loss: 0.9569 - val_accuracy: 0.8380\n",
      "Epoch 135/150\n",
      "1552/1552 [==============================] - 0s 178us/sample - loss: 0.0060 - accuracy: 0.9994 - val_loss: 0.9549 - val_accuracy: 0.8483\n",
      "Epoch 136/150\n",
      "1552/1552 [==============================] - 0s 150us/sample - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.9494 - val_accuracy: 0.8406\n",
      "Epoch 137/150\n",
      "1552/1552 [==============================] - 0s 147us/sample - loss: 0.0084 - accuracy: 0.9994 - val_loss: 0.9557 - val_accuracy: 0.8483\n",
      "Epoch 138/150\n",
      "1552/1552 [==============================] - 0s 131us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.0786 - val_accuracy: 0.8329\n",
      "Epoch 139/150\n",
      "1552/1552 [==============================] - 0s 113us/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.9931 - val_accuracy: 0.8303\n",
      "Epoch 140/150\n",
      "1552/1552 [==============================] - 0s 130us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.9790 - val_accuracy: 0.8201\n",
      "Epoch 141/150\n",
      "1552/1552 [==============================] - 0s 132us/sample - loss: 0.0085 - accuracy: 0.9994 - val_loss: 0.9582 - val_accuracy: 0.8278\n",
      "Epoch 142/150\n",
      "1552/1552 [==============================] - 0s 161us/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0679 - val_accuracy: 0.8380\n",
      "Epoch 143/150\n",
      "1552/1552 [==============================] - 0s 170us/sample - loss: 0.0079 - accuracy: 0.9994 - val_loss: 1.0310 - val_accuracy: 0.8406\n",
      "Epoch 144/150\n",
      "1552/1552 [==============================] - 0s 118us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.9964 - val_accuracy: 0.8380\n",
      "Epoch 145/150\n",
      "1552/1552 [==============================] - 0s 122us/sample - loss: 0.0069 - accuracy: 0.9994 - val_loss: 0.9902 - val_accuracy: 0.8406\n",
      "Epoch 146/150\n",
      "1552/1552 [==============================] - 0s 120us/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.9855 - val_accuracy: 0.8355\n",
      "Epoch 147/150\n",
      "1552/1552 [==============================] - 0s 121us/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.0042 - val_accuracy: 0.8252\n",
      "Epoch 148/150\n",
      "1552/1552 [==============================] - 0s 121us/sample - loss: 0.0078 - accuracy: 0.9994 - val_loss: 0.9840 - val_accuracy: 0.8252\n",
      "Epoch 149/150\n",
      "1552/1552 [==============================] - 0s 158us/sample - loss: 0.0060 - accuracy: 0.9994 - val_loss: 0.9883 - val_accuracy: 0.8278\n",
      "Epoch 150/150\n",
      "1552/1552 [==============================] - 0s 153us/sample - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.9873 - val_accuracy: 0.8303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bd642b6e80>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='relu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "                 #kernel_regularizer=regularizers.l2(0.001),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dropout(0.1),\n",
    "        layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    #layers.Dense(units, activation=activation_func),\n",
    "    #layers.SimpleRNN(100,unroll=True),\n",
    "    layers.Dense(6 ,activation='softmax')\n",
    "])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='sparse_categorical_crossentropy', patience=10)\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.5731299e-03, 5.2631032e-01, 3.7737048e-04, 4.6344689e-01,\n",
       "        2.6438874e-04, 2.7860471e-05],\n",
       "       [6.3374396e-11, 3.8282865e-01, 1.5953320e-06, 3.9615802e-08,\n",
       "        2.1569338e-07, 6.1716950e-01],\n",
       "       [2.9101342e-15, 3.8773679e-10, 3.9593419e-11, 1.0189453e-08,\n",
       "        3.3719264e-12, 1.0000000e+00],\n",
       "       ...,\n",
       "       [2.6858095e-14, 1.0029092e-07, 6.0756061e-10, 4.4949973e-09,\n",
       "        2.7131176e-11, 9.9999988e-01],\n",
       "       [5.1457187e-14, 1.8195551e-09, 4.3263088e-10, 5.9100960e-09,\n",
       "        1.1759822e-11, 1.0000000e+00],\n",
       "       [7.5174970e-15, 9.7326648e-01, 1.7150293e-09, 3.3121278e-10,\n",
       "        1.9109669e-09, 2.6733540e-02]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "y_pred_keras = model2.predict(X_test)[:, :]\n",
    "y_pred_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 5, 0, 5, 5, 0, 1, 1, 1, 0, 4, 4, 1, 4, 5, 5, 1, 1, 5, 4, 0,\n",
       "       1, 0, 0, 5, 4, 0, 1, 5, 0, 5, 0, 5, 4, 5, 5, 3, 4, 4, 3, 1, 1, 0,\n",
       "       5, 1, 4, 1, 1, 0, 1, 0, 5, 0, 1, 0, 0, 0, 1, 5, 1, 5, 1, 1, 4, 0,\n",
       "       0, 5, 1, 3, 4, 1, 4, 1, 0, 1, 4, 1, 1, 0, 1, 0, 1, 0, 1, 1, 5, 4,\n",
       "       2, 0, 0, 0, 1, 5, 0, 1, 0, 1, 3, 0, 1, 1, 0, 3, 1, 0, 0, 1, 1, 4,\n",
       "       1, 1, 3, 1, 0, 5, 4, 4, 0, 5, 0, 5, 0, 5, 1, 1, 5, 5, 1, 0, 4, 1,\n",
       "       4, 1, 1, 0, 4, 0, 0, 5, 1, 1, 1, 3, 1, 1, 5, 0, 5, 1, 5, 1, 1, 0,\n",
       "       1, 5, 1, 0, 5, 1, 0, 4, 0, 4, 1, 5, 0, 0, 3, 4, 3, 5, 1, 5, 2, 4,\n",
       "       1, 1, 1, 1, 1, 0, 5, 1, 1, 1, 0, 1, 1, 0, 3, 1, 5, 1, 0, 5, 3, 1,\n",
       "       0, 4, 1, 1, 3, 1, 0, 5, 0, 5, 5, 5, 1, 4, 4, 1, 1, 5, 1, 4, 1, 4,\n",
       "       1, 1, 0, 3, 1, 1, 1, 1, 1, 3, 0, 0, 5, 1, 2, 1, 5, 4, 0, 1, 0, 4,\n",
       "       1, 4, 3, 1, 4, 1, 1, 1, 5, 1, 0, 1, 1, 5, 1, 5, 1, 2, 5, 5, 4, 1,\n",
       "       0, 1, 5, 1, 0, 1, 4, 1, 3, 2, 0, 1, 1, 2, 3, 0, 3, 5, 0, 1, 1, 1,\n",
       "       5, 0, 1, 0, 1, 1, 3, 3, 0, 5, 3, 5, 0, 0, 5, 1, 4, 0, 4, 4, 4, 1,\n",
       "       5, 0, 0, 3, 5, 4, 4, 0, 5, 5, 5, 0, 0, 1, 1, 0, 3, 4, 0, 1, 5, 1,\n",
       "       1, 5, 5, 5, 4, 1, 1, 4, 5, 5, 1, 0, 0, 1, 3, 5, 1, 0, 1, 0, 1, 1,\n",
       "       1, 3, 0, 3, 0, 5, 4, 1, 5, 0, 1, 5, 2, 0, 5, 5, 1, 1, 1, 0, 0, 0,\n",
       "       5, 4, 4, 5, 5, 0, 5, 3, 1, 0, 4, 1, 5, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = np.argmax(y_pred_keras, axis=1)\n",
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 5, 0, 5, 5, 3, 1, 1, 1, 0, 4, 4, 1, 4, 1, 5, 1, 1, 5, 4, 0,\n",
       "       1, 0, 0, 5, 4, 0, 1, 1, 0, 5, 0, 5, 4, 5, 5, 1, 4, 4, 3, 1, 1, 0,\n",
       "       5, 1, 4, 1, 1, 0, 1, 0, 1, 0, 1, 3, 1, 0, 1, 5, 3, 5, 5, 1, 4, 0,\n",
       "       1, 5, 1, 0, 4, 1, 4, 1, 0, 1, 4, 1, 1, 0, 5, 0, 1, 0, 1, 5, 5, 1,\n",
       "       2, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 4,\n",
       "       3, 1, 1, 1, 0, 5, 4, 4, 0, 5, 0, 5, 0, 5, 1, 5, 5, 5, 1, 0, 1, 1,\n",
       "       4, 1, 1, 0, 4, 0, 1, 5, 1, 1, 1, 3, 1, 1, 5, 0, 5, 4, 1, 1, 1, 0,\n",
       "       1, 5, 1, 0, 5, 4, 0, 4, 4, 4, 1, 5, 0, 0, 3, 1, 3, 5, 1, 5, 2, 4,\n",
       "       1, 1, 1, 0, 1, 0, 5, 5, 1, 1, 0, 1, 1, 0, 4, 1, 3, 3, 1, 5, 3, 3,\n",
       "       0, 4, 1, 1, 0, 1, 0, 5, 0, 5, 5, 5, 5, 4, 1, 3, 1, 5, 1, 4, 1, 4,\n",
       "       1, 1, 0, 3, 1, 1, 1, 1, 1, 3, 0, 0, 5, 1, 2, 0, 5, 4, 0, 1, 0, 4,\n",
       "       1, 4, 3, 1, 1, 1, 5, 1, 5, 1, 0, 1, 1, 5, 5, 5, 1, 2, 5, 5, 4, 2,\n",
       "       0, 3, 5, 5, 0, 5, 4, 1, 3, 2, 0, 1, 1, 2, 3, 0, 5, 5, 0, 1, 1, 1,\n",
       "       5, 1, 1, 0, 1, 5, 3, 3, 0, 5, 4, 5, 0, 0, 5, 1, 4, 4, 4, 1, 4, 5,\n",
       "       5, 0, 0, 3, 5, 4, 1, 0, 5, 5, 5, 0, 0, 1, 1, 0, 5, 4, 0, 0, 5, 1,\n",
       "       0, 5, 1, 5, 4, 1, 1, 4, 5, 5, 1, 0, 0, 1, 3, 5, 1, 0, 0, 1, 1, 1,\n",
       "       1, 4, 0, 3, 0, 5, 4, 1, 5, 0, 1, 5, 2, 0, 5, 5, 1, 1, 1, 0, 0, 0,\n",
       "       5, 0, 4, 5, 5, 3, 5, 3, 0, 0, 3, 1, 5, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2,  0,  0,  0,  0,  0, -3,  0,  0,  0,  0,  0,  0,  0,  0,  4,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,\n",
       "        0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  4,  0,  0, -3, -1,  0,  0,  0, -2,  0, -4,  0,  0,  0, -1,  0,\n",
       "        0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -4,  0,  0,  0,  0,\n",
       "       -4,  0,  3,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  2,  0,  0,  0,\n",
       "        0,  3,  0,  0,  0,  0,  0,  0, -2,  0,  2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0, -4,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,\n",
       "        0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -3,  4,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0, -3,  0,  0, -4,  0,  0,  0,  0,  0,  0,  3,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -4,  0,  0,  0,\n",
       "        0,  0,  0, -1,  0,  2, -2, -1,  0,  0, -2,  0,  0,  0,  0,  3,  0,\n",
       "        0,  0,  0,  0,  0,  0, -4,  0,  3, -2,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  3,  0, -4,  0,  0,  0,  0,  0,  0,\n",
       "        0, -4,  0,  0,  0,  0,  0,  0, -1,  0, -2,  0, -4,  0, -4,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0, -4,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0, -4,  0,  3,\n",
       "        0, -4,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0, -2,  0,  0,  1,  0,  0,  1,  0,  4,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  4,  0,  0,  0, -3,  0,  0,  1,  0,  1,  0,  0,  0,  0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303341902313625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((max_index-y_test)==0)/X_test.shape[0] #verify the method to calculate accuracy, confirm the number matches val loss output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Hold Out dataset (new papers, no label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Synthesis of virus-specific RNA in permeabiliz...</td>\n",
       "      <td>Abstract We have developed a permeabilized cel...</td>\n",
       "      <td>-0.049652</td>\n",
       "      <td>-0.011094</td>\n",
       "      <td>-0.026801</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>0.042196</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>0.020746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056881</td>\n",
       "      <td>-0.070640</td>\n",
       "      <td>-0.017600</td>\n",
       "      <td>-0.070502</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>-0.008354</td>\n",
       "      <td>0.054538</td>\n",
       "      <td>-0.007377</td>\n",
       "      <td>-0.035350</td>\n",
       "      <td>-0.052909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Primary structure and post-translational proce...</td>\n",
       "      <td>Abstract The nucleotide sequence of the peplom...</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>-0.040164</td>\n",
       "      <td>-0.030494</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.061669</td>\n",
       "      <td>-0.012344</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051405</td>\n",
       "      <td>-0.065164</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>-0.065149</td>\n",
       "      <td>0.049234</td>\n",
       "      <td>-0.030314</td>\n",
       "      <td>0.048964</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>-0.028852</td>\n",
       "      <td>-0.064704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comparison of the genome organization of toro-...</td>\n",
       "      <td>Abstract Recently, toroviruses and coronavirus...</td>\n",
       "      <td>-0.036349</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.050869</td>\n",
       "      <td>-0.034782</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.060601</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>-0.063528</td>\n",
       "      <td>-0.059635</td>\n",
       "      <td>-0.063528</td>\n",
       "      <td>0.034254</td>\n",
       "      <td>-0.030137</td>\n",
       "      <td>0.043699</td>\n",
       "      <td>-0.005431</td>\n",
       "      <td>0.057524</td>\n",
       "      <td>-0.056803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The complete sequence (22 kilobases) of murine...</td>\n",
       "      <td>Abstract The 5′-most gene, gene 1, of the geno...</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>-0.013924</td>\n",
       "      <td>-0.027915</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>0.058013</td>\n",
       "      <td>-0.036737</td>\n",
       "      <td>0.023549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057039</td>\n",
       "      <td>-0.058295</td>\n",
       "      <td>-0.052740</td>\n",
       "      <td>-0.058295</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>-0.054457</td>\n",
       "      <td>0.053144</td>\n",
       "      <td>-0.055570</td>\n",
       "      <td>0.049414</td>\n",
       "      <td>-0.057258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>De novo generation of defective interfering RN...</td>\n",
       "      <td>Abstract Defective interfering (DI) RNAs were ...</td>\n",
       "      <td>-0.049280</td>\n",
       "      <td>-0.035973</td>\n",
       "      <td>-0.050783</td>\n",
       "      <td>-0.031401</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>0.059310</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010444</td>\n",
       "      <td>-0.062296</td>\n",
       "      <td>-0.037243</td>\n",
       "      <td>-0.062295</td>\n",
       "      <td>-0.053315</td>\n",
       "      <td>-0.030183</td>\n",
       "      <td>0.038881</td>\n",
       "      <td>-0.047072</td>\n",
       "      <td>0.060653</td>\n",
       "      <td>-0.056800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Synthesis of virus-specific RNA in permeabiliz...   \n",
       "1  Primary structure and post-translational proce...   \n",
       "2  Comparison of the genome organization of toro-...   \n",
       "3  The complete sequence (22 kilobases) of murine...   \n",
       "4  De novo generation of defective interfering RN...   \n",
       "\n",
       "                                            abstract         0         1  \\\n",
       "0  Abstract We have developed a permeabilized cel... -0.049652 -0.011094   \n",
       "1  Abstract The nucleotide sequence of the peplom...  0.002379  0.018184   \n",
       "2  Abstract Recently, toroviruses and coronavirus... -0.036349 -0.000761   \n",
       "3  Abstract The 5′-most gene, gene 1, of the geno...  0.007206 -0.013924   \n",
       "4  Abstract Defective interfering (DI) RNAs were ... -0.049280 -0.035973   \n",
       "\n",
       "          2         3         4         5         6         7  ...       502  \\\n",
       "0 -0.026801 -0.002311  0.042196  0.021205 -0.003560  0.020746  ... -0.056881   \n",
       "1 -0.040164 -0.030494  0.008339  0.061669 -0.012344 -0.002478  ... -0.051405   \n",
       "2 -0.050869 -0.034782  0.035800  0.060601  0.005766 -0.029210  ...  0.035874   \n",
       "3 -0.027915  0.040264  0.007304  0.058013 -0.036737  0.023549  ... -0.057039   \n",
       "4 -0.050783 -0.031401  0.031795  0.059310  0.052120  0.020553  ... -0.010444   \n",
       "\n",
       "        503       504       505       506       507       508       509  \\\n",
       "0 -0.070640 -0.017600 -0.070502 -0.000305 -0.008354  0.054538 -0.007377   \n",
       "1 -0.065164 -0.010036 -0.065149  0.049234 -0.030314  0.048964  0.011794   \n",
       "2 -0.063528 -0.059635 -0.063528  0.034254 -0.030137  0.043699 -0.005431   \n",
       "3 -0.058295 -0.052740 -0.058295  0.014025 -0.054457  0.053144 -0.055570   \n",
       "4 -0.062296 -0.037243 -0.062295 -0.053315 -0.030183  0.038881 -0.047072   \n",
       "\n",
       "        510       511  \n",
       "0 -0.035350 -0.052909  \n",
       "1 -0.028852 -0.064704  \n",
       "2  0.057524 -0.056803  \n",
       "3  0.049414 -0.057258  \n",
       "4  0.060653 -0.056800  \n",
       "\n",
       "[5 rows x 514 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2= pd.read_csv('HoldOut388_512vectors2WithTitles.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.049652</td>\n",
       "      <td>-0.011094</td>\n",
       "      <td>-0.026801</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>0.042196</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>0.020746</td>\n",
       "      <td>0.045434</td>\n",
       "      <td>0.025883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056881</td>\n",
       "      <td>-0.070640</td>\n",
       "      <td>-0.017600</td>\n",
       "      <td>-0.070502</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>-0.008354</td>\n",
       "      <td>0.054538</td>\n",
       "      <td>-0.007377</td>\n",
       "      <td>-0.035350</td>\n",
       "      <td>-0.052909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>-0.040164</td>\n",
       "      <td>-0.030494</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.061669</td>\n",
       "      <td>-0.012344</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>0.036577</td>\n",
       "      <td>0.026191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051405</td>\n",
       "      <td>-0.065164</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>-0.065149</td>\n",
       "      <td>0.049234</td>\n",
       "      <td>-0.030314</td>\n",
       "      <td>0.048964</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>-0.028852</td>\n",
       "      <td>-0.064704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.036349</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.050869</td>\n",
       "      <td>-0.034782</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.060601</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.053824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>-0.063528</td>\n",
       "      <td>-0.059635</td>\n",
       "      <td>-0.063528</td>\n",
       "      <td>0.034254</td>\n",
       "      <td>-0.030137</td>\n",
       "      <td>0.043699</td>\n",
       "      <td>-0.005431</td>\n",
       "      <td>0.057524</td>\n",
       "      <td>-0.056803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007206</td>\n",
       "      <td>-0.013924</td>\n",
       "      <td>-0.027915</td>\n",
       "      <td>0.040264</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>0.058013</td>\n",
       "      <td>-0.036737</td>\n",
       "      <td>0.023549</td>\n",
       "      <td>0.011380</td>\n",
       "      <td>0.048981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057039</td>\n",
       "      <td>-0.058295</td>\n",
       "      <td>-0.052740</td>\n",
       "      <td>-0.058295</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>-0.054457</td>\n",
       "      <td>0.053144</td>\n",
       "      <td>-0.055570</td>\n",
       "      <td>0.049414</td>\n",
       "      <td>-0.057258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.049280</td>\n",
       "      <td>-0.035973</td>\n",
       "      <td>-0.050783</td>\n",
       "      <td>-0.031401</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>0.059310</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.057714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010444</td>\n",
       "      <td>-0.062296</td>\n",
       "      <td>-0.037243</td>\n",
       "      <td>-0.062295</td>\n",
       "      <td>-0.053315</td>\n",
       "      <td>-0.030183</td>\n",
       "      <td>0.038881</td>\n",
       "      <td>-0.047072</td>\n",
       "      <td>0.060653</td>\n",
       "      <td>-0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.058478</td>\n",
       "      <td>-0.004469</td>\n",
       "      <td>-0.063835</td>\n",
       "      <td>0.006031</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>-0.063798</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>-0.025551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015296</td>\n",
       "      <td>-0.067067</td>\n",
       "      <td>-0.012751</td>\n",
       "      <td>-0.067071</td>\n",
       "      <td>-0.028087</td>\n",
       "      <td>-0.046303</td>\n",
       "      <td>0.056527</td>\n",
       "      <td>-0.058321</td>\n",
       "      <td>-0.019072</td>\n",
       "      <td>-0.036170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>-0.048442</td>\n",
       "      <td>-0.035224</td>\n",
       "      <td>-0.057540</td>\n",
       "      <td>-0.035870</td>\n",
       "      <td>0.056764</td>\n",
       "      <td>0.058271</td>\n",
       "      <td>0.055362</td>\n",
       "      <td>-0.055081</td>\n",
       "      <td>0.045430</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032115</td>\n",
       "      <td>-0.061738</td>\n",
       "      <td>-0.052958</td>\n",
       "      <td>-0.061738</td>\n",
       "      <td>0.044325</td>\n",
       "      <td>-0.048101</td>\n",
       "      <td>0.059945</td>\n",
       "      <td>-0.059864</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.052417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>-0.038116</td>\n",
       "      <td>-0.015935</td>\n",
       "      <td>-0.051716</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.058031</td>\n",
       "      <td>0.043148</td>\n",
       "      <td>-0.037475</td>\n",
       "      <td>0.028737</td>\n",
       "      <td>0.049610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040447</td>\n",
       "      <td>-0.061073</td>\n",
       "      <td>-0.030382</td>\n",
       "      <td>-0.061073</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>-0.007703</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>-0.058802</td>\n",
       "      <td>-0.046988</td>\n",
       "      <td>-0.056345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>-0.055184</td>\n",
       "      <td>0.056014</td>\n",
       "      <td>-0.050239</td>\n",
       "      <td>0.027768</td>\n",
       "      <td>-0.047458</td>\n",
       "      <td>-0.010766</td>\n",
       "      <td>0.043399</td>\n",
       "      <td>-0.054596</td>\n",
       "      <td>-0.028848</td>\n",
       "      <td>-0.024939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037741</td>\n",
       "      <td>-0.056081</td>\n",
       "      <td>-0.044042</td>\n",
       "      <td>-0.056081</td>\n",
       "      <td>0.054785</td>\n",
       "      <td>-0.054763</td>\n",
       "      <td>0.039602</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>-0.056081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.049279</td>\n",
       "      <td>-0.059444</td>\n",
       "      <td>-0.058115</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>-0.035380</td>\n",
       "      <td>0.058563</td>\n",
       "      <td>-0.054313</td>\n",
       "      <td>0.057236</td>\n",
       "      <td>-0.007523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059849</td>\n",
       "      <td>-0.062269</td>\n",
       "      <td>-0.055467</td>\n",
       "      <td>-0.062269</td>\n",
       "      <td>0.023908</td>\n",
       "      <td>-0.041097</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>-0.033178</td>\n",
       "      <td>-0.048842</td>\n",
       "      <td>-0.040750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.049652 -0.011094 -0.026801 -0.002311  0.042196  0.021205 -0.003560   \n",
       "1    0.002379  0.018184 -0.040164 -0.030494  0.008339  0.061669 -0.012344   \n",
       "2   -0.036349 -0.000761 -0.050869 -0.034782  0.035800  0.060601  0.005766   \n",
       "3    0.007206 -0.013924 -0.027915  0.040264  0.007304  0.058013 -0.036737   \n",
       "4   -0.049280 -0.035973 -0.050783 -0.031401  0.031795  0.059310  0.052120   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "383  0.000114  0.058478 -0.004469 -0.063835  0.006031  0.020011  0.008401   \n",
       "384 -0.048442 -0.035224 -0.057540 -0.035870  0.056764  0.058271  0.055362   \n",
       "385 -0.038116 -0.015935 -0.051716  0.026050  0.053225  0.058031  0.043148   \n",
       "386 -0.055184  0.056014 -0.050239  0.027768 -0.047458 -0.010766  0.043399   \n",
       "387 -0.013489  0.049279 -0.059444 -0.058115 -0.059752 -0.035380  0.058563   \n",
       "\n",
       "            7         8         9  ...       502       503       504  \\\n",
       "0    0.020746  0.045434  0.025883  ... -0.056881 -0.070640 -0.017600   \n",
       "1   -0.002478  0.036577  0.026191  ... -0.051405 -0.065164 -0.010036   \n",
       "2   -0.029210  0.022040  0.053824  ...  0.035874 -0.063528 -0.059635   \n",
       "3    0.023549  0.011380  0.048981  ... -0.057039 -0.058295 -0.052740   \n",
       "4    0.020553  0.001214  0.057714  ... -0.010444 -0.062296 -0.037243   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "383 -0.063798  0.057093 -0.025551  ... -0.015296 -0.067067 -0.012751   \n",
       "384 -0.055081  0.045430  0.043651  ... -0.032115 -0.061738 -0.052958   \n",
       "385 -0.037475  0.028737  0.049610  ... -0.040447 -0.061073 -0.030382   \n",
       "386 -0.054596 -0.028848 -0.024939  ... -0.037741 -0.056081 -0.044042   \n",
       "387 -0.054313  0.057236 -0.007523  ... -0.059849 -0.062269 -0.055467   \n",
       "\n",
       "          505       506       507       508       509       510       511  \n",
       "0   -0.070502 -0.000305 -0.008354  0.054538 -0.007377 -0.035350 -0.052909  \n",
       "1   -0.065149  0.049234 -0.030314  0.048964  0.011794 -0.028852 -0.064704  \n",
       "2   -0.063528  0.034254 -0.030137  0.043699 -0.005431  0.057524 -0.056803  \n",
       "3   -0.058295  0.014025 -0.054457  0.053144 -0.055570  0.049414 -0.057258  \n",
       "4   -0.062295 -0.053315 -0.030183  0.038881 -0.047072  0.060653 -0.056800  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "383 -0.067071 -0.028087 -0.046303  0.056527 -0.058321 -0.019072 -0.036170  \n",
       "384 -0.061738  0.044325 -0.048101  0.059945 -0.059864 -0.048209 -0.052417  \n",
       "385 -0.061073  0.006920 -0.007703  0.049406 -0.058802 -0.046988 -0.056345  \n",
       "386 -0.056081  0.054785 -0.054763  0.039602  0.006800  0.003140 -0.056081  \n",
       "387 -0.062269  0.023908 -0.041097  0.012535 -0.033178 -0.048842 -0.040750  \n",
       "\n",
       "[388 rows x 512 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imputed2=df2.drop(['title', 'abstract'], axis=1)\n",
    "df_imputed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 512)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test2=df_imputed2\n",
    "x_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.6764970e-13, 1.9692442e-04, 5.6723306e-09, 9.6481312e-09,\n",
       "        4.5470461e-08, 9.9980301e-01],\n",
       "       [6.0039992e-14, 1.7254754e-06, 1.3729931e-09, 6.3185183e-08,\n",
       "        9.7330775e-11, 9.9999821e-01],\n",
       "       [3.2728131e-15, 4.1799685e-11, 2.1057968e-11, 5.0903433e-09,\n",
       "        2.1310488e-12, 1.0000000e+00],\n",
       "       ...,\n",
       "       [1.5166425e-10, 8.4924528e-07, 1.1948390e-07, 2.5660620e-06,\n",
       "        1.7249308e-08, 9.9999642e-01],\n",
       "       [3.2596505e-01, 1.5062708e-01, 4.2049721e-04, 5.2298576e-01,\n",
       "        9.2161463e-07, 6.6337554e-07],\n",
       "       [9.9999452e-01, 1.4166167e-06, 2.2941456e-06, 1.6106654e-06,\n",
       "        1.3691961e-07, 2.5797693e-09]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_keras2 = model2.predict(x_test2)[:, :]\n",
    "y_pred_keras2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n",
       "       1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 1, 5, 5,\n",
       "       4, 4, 1, 1, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 3, 5, 5,\n",
       "       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 4, 4, 1, 4, 5, 1, 1, 5, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n",
       "       4, 1, 0, 4, 4, 1, 4, 4, 3, 0, 1, 3, 3, 4, 1, 0, 5, 1, 3, 0, 3, 1,\n",
       "       0, 5, 1, 5, 3, 1, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n",
       "       1, 0, 0, 3, 5, 2, 0, 1, 1, 0, 4, 1, 1, 0, 0, 5, 1, 1, 0, 0, 5, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n",
       "       5, 0, 1, 1, 5, 1, 1, 5, 2, 5, 5, 0, 5, 1, 3, 0, 1, 4, 0, 1, 1, 5,\n",
       "       5, 0, 1, 1, 5, 0, 5, 1, 2, 3, 1, 0, 1, 5, 1, 4, 5, 1, 5, 1, 2, 5,\n",
       "       1, 5, 0, 0, 1, 2, 5, 0, 0, 5, 1, 5, 2, 5, 5, 1, 5, 5, 5, 3, 0, 1,\n",
       "       4, 1, 5, 5, 1, 5, 5, 1, 5, 1, 1, 1, 5, 4, 0, 5, 1, 5, 1, 2, 2, 0,\n",
       "       1, 1, 5, 5, 1, 5, 5, 3, 2, 4, 1, 1, 5, 5, 0, 5, 1, 2, 1, 5, 1, 2,\n",
       "       5, 5, 1, 2, 5, 4, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 1, 1, 3, 3, 3,\n",
       "       5, 5, 5, 1, 5, 5, 1, 1, 5, 5, 1, 5, 5, 1, 1, 5, 5, 5, 5, 5, 5, 1,\n",
       "       5, 5, 5, 1, 5, 5, 0, 1, 1, 3, 1, 1, 5, 1, 5, 5, 2, 5, 2, 5, 1, 1,\n",
       "       5, 5, 3, 3, 5, 5, 5, 1, 1, 1, 5, 5, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indexMLP = np.argmax(y_pred_keras2, axis=1)\n",
    "max_indexMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation method 2\n",
    "## Compare MLP with RF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare MLP with RF predictions\n",
    "\n",
    "# RF results:\n",
    "# preds[0:20] \n",
    "# Out[48]:\n",
    "# array([[0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 1, 0],\n",
    "#        [0, 0, 0, 0, 1, 0],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1]], dtype=uint8)\n",
    "# [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5],\n",
    "\n",
    "# MLP results 1-20\n",
    "# [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the two methods, the first 20 predictions match perfectly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF preds[21:40]\n",
    "\n",
    "# Out[51]:\n",
    "# array([[0, 1, 0, 0, 0, 0],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 1, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 1, 0],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 1, 0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0, 0, 1],\n",
    "#        [0, 0, 0, 0, 1, 0]], dtype=uint8)\n",
    "#([1, 1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4],\n",
    "\n",
    "# MLP results 21-40\n",
    "# 5, 1, 1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indexRF=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n",
    "       1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 1, 5, 5,\n",
    "       1, 4, 1, 0, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 3, 5, 5,\n",
    "       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 1,\n",
    "       1, 1, 1, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n",
    "       4, 1, 0, 4, 4, 1, 4, 4, 3, 0, 1, 0, 0, 4, 1, 0, 5, 1, 3, 1, 3, 1,\n",
    "       0, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n",
    "       1, 0, 0, 3, 5, 2, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 1, 1, 0, 0, 5, 1,\n",
    "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n",
    "       5, 0, 5, 1, 5, 1, 1, 5, 1, 5, 1, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
    "       5, 1, 1, 1, 5, 0, 5, 1, 1, 0, 1, 0, 1, 5, 1, 4, 5, 1, 0, 1, 1, 5,\n",
    "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 5, 5, 5, 0, 0, 1,\n",
    "       4, 1, 5, 5, 0, 0, 5, 1, 5, 5, 1, 0, 5, 4, 0, 5, 1, 5, 1, 1, 1, 0,\n",
    "       1, 1, 5, 5, 1, 5, 5, 1, 1, 0, 1, 0, 0, 5, 0, 5, 1, 1, 1, 5, 0, 0,\n",
    "       5, 0, 0, 0, 5, 1, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 1, 1, 0, 0, 5,\n",
    "       1, 5, 5, 1, 5, 0, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 0, 0, 5, 1,\n",
    "       1, 0, 0, 1, 5, 0, 0, 1, 1, 5, 1, 5, 5, 1, 5, 5, 0, 0, 0, 5, 1, 1,\n",
    "       0, 0, 0, 4, 5, 0, 5, 1, 0, 1, 0, 5, 0, 0]\n",
    "max_indexRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  1,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0, -3,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  3,  3,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
       "        0, -4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -4,  0,  0,  0,\n",
       "        0,  0,  1,  0,  4,  0,  0,  1,  3,  0,  0,  4,  0,  0,  0,  5,  0,\n",
       "       -1,  0,  0,  0,  0,  0,  0,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        5,  0,  1,  0,  0,  4,  0, -1,  0,  1,  5,  0,  0,  5,  1,  0,  2,\n",
       "        0,  5,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  1,  5,  0,  0,\n",
       "        0, -4,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  2,  1,  4,  0,  1,  5,  0,  0,  0,  0,  1,  0,  0,\n",
       "        1,  2,  0,  5,  1,  2,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  3,  3, -2,  4,  0,  0,  0,  0,  5,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, -4,  0,  0,  0,  5,  5,  0,  0,  4,  5,  5,  0,  0,\n",
       "        5,  0,  0,  0, -2,  0, -4,  0,  0,  0,  0,  2,  5,  2,  0,  0,  0,\n",
       "        5,  5,  3, -1,  0,  5,  0,  0,  1,  0,  5,  0,  3,  0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indexMLP-max_indexRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well MLP and RF predictions match?\n",
    "### 0.8144329896907216\n",
    "### 81.4%\n",
    "\n",
    "### If we have a third algo results, we can have a ensemble model with majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8144329896907216"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((max_indexMLP-max_indexRF)==0)/x_test2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Therefore, I run a GXB. The result is as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indexXGB=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n",
    "       1, 5, 5, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 4, 5, 5,\n",
    "       1, 4, 1, 2, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 1, 5, 5,\n",
    "       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 5,\n",
    "       5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 5, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n",
    "       4, 1, 0, 4, 4, 1, 4, 4, 1, 0, 1, 4, 3, 4, 1, 0, 1, 1, 5, 1, 3, 1,\n",
    "       1, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n",
    "       1, 0, 0, 1, 5, 1, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 0, 1, 0, 0, 5, 1,\n",
    "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n",
    "       5, 0, 5, 1, 5, 1, 1, 1, 2, 5, 5, 0, 5, 1, 3, 0, 1, 4, 0, 1, 1, 5,\n",
    "       5, 1, 1, 1, 5, 0, 5, 1, 1, 3, 1, 0, 1, 5, 1, 4, 5, 1, 5, 1, 1, 5,\n",
    "       1, 5, 0, 0, 1, 2, 5, 0, 0, 5, 1, 5, 2, 5, 1, 1, 5, 5, 5, 1, 0, 1,\n",
    "       4, 1, 5, 5, 5, 5, 5, 1, 5, 1, 1, 1, 5, 4, 0, 5, 1, 5, 1, 2, 2, 0,\n",
    "       1, 1, 5, 5, 1, 5, 5, 5, 2, 4, 1, 5, 1, 5, 0, 5, 1, 2, 1, 5, 5, 2,\n",
    "       5, 5, 1, 2, 5, 4, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 5, 1, 3, 3, 5,\n",
    "       5, 5, 5, 1, 5, 5, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 1,\n",
    "       1, 5, 5, 1, 5, 5, 0, 1, 1, 3, 1, 5, 5, 1, 5, 5, 2, 5, 2, 5, 1, 1,\n",
    "       5, 5, 3, 4, 5, 5, 5, 1, 3, 1, 5, 5, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8943298969072165"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((max_indexMLP-max_indexXGB)==0)/x_test2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB predictions match well with MLP predictions with 89.4% agreement.\n",
    "#### 0.8943298969072165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-243950efddf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_indexRF\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmax_indexXGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx_test2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "sum((max_indexRF-max_indexXGB)==0)/x_test2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max_indexMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max_indexRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max_indexXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npRF= np.asarray(max_indexRF, dtype=np.float32)\n",
    "type(npRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8118556701030928"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((npRF-max_indexXGB)==0)/x_test2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB vs RF: agree on 81.2% of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -4.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0., -3.,  0.,  0.,  0.,  0.,  0., -2.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -4., -4.,  0.,  0.,\n",
       "        0.,  0.,  0., -1.,  0.,  0.,  0., -4.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  2.,  0.,  0., -4., -3.,  0.,  0.,  0.,  4.,  0., -2.,  0.,\n",
       "        0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  2.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4., -1.,  0.,\n",
       "       -4.,  0.,  0., -1., -3.,  0.,  0., -4.,  0.,  0.,  0., -5.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -3.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0., -5.,  0.,  0.,  0.,  0., -4.,  0.,  1.,  0.,\n",
       "       -1., -5.,  0.,  0., -5., -1.,  0., -2.,  0., -1.,  0.,  0.,  0.,\n",
       "        0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -5., -5.,  0.,  0.,  0.,\n",
       "        4.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0., -4., -1., -4.,  0., -5., -1.,\n",
       "        0.,  0.,  0.,  0., -1.,  0.,  0., -5., -2.,  0., -5., -1., -2.,\n",
       "        0., -3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "       -4.,  0., -3., -3.,  0., -4.,  0.,  0.,  0.,  0., -5.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -5., -5.,  0.,\n",
       "        0.,  0., -5., -5.,  0.,  0., -5.,  0.,  0.,  0.,  2.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0., -2., -5., -2.,  0.,  0.,  0., -5., -5., -3.,\n",
       "        0.,  0., -5.,  0.,  0., -3.,  0., -5.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npRF-max_indexXGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote\n",
    "### For this purpose, I will stack these three predictions and count the mode of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = np.array([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n",
    "       1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 1, 5, 5,\n",
    "       4, 4, 1, 1, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 3, 5, 5,\n",
    "       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 4, 4, 1, 4, 5, 1, 1, 5, 1,\n",
    "       1, 1, 1, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n",
    "       4, 1, 0, 4, 4, 1, 4, 4, 3, 0, 1, 3, 3, 4, 1, 0, 5, 1, 3, 0, 3, 1,\n",
    "       0, 5, 1, 5, 3, 1, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n",
    "       1, 0, 0, 3, 5, 2, 0, 1, 1, 0, 4, 1, 1, 0, 0, 5, 1, 1, 0, 0, 5, 1,\n",
    "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n",
    "       5, 0, 1, 1, 5, 1, 1, 5, 2, 5, 5, 0, 5, 1, 3, 0, 1, 4, 0, 1, 1, 5,\n",
    "       5, 0, 1, 1, 5, 0, 5, 1, 2, 3, 1, 0, 1, 5, 1, 4, 5, 1, 5, 1, 2, 5,\n",
    "       1, 5, 0, 0, 1, 2, 5, 0, 0, 5, 1, 5, 2, 5, 5, 1, 5, 5, 5, 3, 0, 1,\n",
    "       4, 1, 5, 5, 1, 5, 5, 1, 5, 1, 1, 1, 5, 4, 0, 5, 1, 5, 1, 2, 2, 0,\n",
    "       1, 1, 5, 5, 1, 5, 5, 3, 2, 4, 1, 1, 5, 5, 0, 5, 1, 2, 1, 5, 1, 2,\n",
    "       5, 5, 1, 2, 5, 4, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 1, 1, 3, 3, 3,\n",
    "       5, 5, 5, 1, 5, 5, 1, 1, 5, 5, 1, 5, 5, 1, 1, 5, 5, 5, 5, 5, 5, 1,\n",
    "       5, 5, 5, 1, 5, 5, 0, 1, 1, 3, 1, 1, 5, 1, 5, 5, 2, 5, 2, 5, 1, 1,\n",
    "       5, 5, 3, 3, 5, 5, 5, 1, 1, 1, 5, 5, 3, 0], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n",
    "       1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 1, 5, 5,\n",
    "       1, 4, 1, 0, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 3, 5, 5,\n",
    "       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 1,\n",
    "       1, 1, 1, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n",
    "       4, 1, 0, 4, 4, 1, 4, 4, 3, 0, 1, 0, 0, 4, 1, 0, 5, 1, 3, 1, 3, 1,\n",
    "       0, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n",
    "       1, 0, 0, 3, 5, 2, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 1, 1, 0, 0, 5, 1,\n",
    "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n",
    "       5, 0, 5, 1, 5, 1, 1, 5, 1, 5, 1, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
    "       5, 1, 1, 1, 5, 0, 5, 1, 1, 0, 1, 0, 1, 5, 1, 4, 5, 1, 0, 1, 1, 5,\n",
    "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 5, 5, 5, 0, 0, 1,\n",
    "       4, 1, 5, 5, 0, 0, 5, 1, 5, 5, 1, 0, 5, 4, 0, 5, 1, 5, 1, 1, 1, 0,\n",
    "       1, 1, 5, 5, 1, 5, 5, 1, 1, 0, 1, 0, 0, 5, 0, 5, 1, 1, 1, 5, 0, 0,\n",
    "       5, 0, 0, 0, 5, 1, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 1, 1, 0, 0, 5,\n",
    "       1, 5, 5, 1, 5, 0, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 0, 0, 5, 1,\n",
    "       1, 0, 0, 1, 5, 0, 0, 1, 1, 5, 1, 5, 5, 1, 5, 5, 0, 0, 0, 5, 1, 1,\n",
    "       0, 0, 0, 4, 5, 0, 5, 1, 0, 1, 0, 5, 0, 0],[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n",
    "       1, 5, 5, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 4, 5, 5,\n",
    "       1, 4, 1, 2, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 1, 5, 5,\n",
    "       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 5,\n",
    "       5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 5, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n",
    "       4, 1, 0, 4, 4, 1, 4, 4, 1, 0, 1, 4, 3, 4, 1, 0, 1, 1, 5, 1, 3, 1,\n",
    "       1, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n",
    "       1, 0, 0, 1, 5, 1, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 0, 1, 0, 0, 5, 1,\n",
    "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n",
    "       5, 0, 5, 1, 5, 1, 1, 1, 2, 5, 5, 0, 5, 1, 3, 0, 1, 4, 0, 1, 1, 5,\n",
    "       5, 1, 1, 1, 5, 0, 5, 1, 1, 3, 1, 0, 1, 5, 1, 4, 5, 1, 5, 1, 1, 5,\n",
    "       1, 5, 0, 0, 1, 2, 5, 0, 0, 5, 1, 5, 2, 5, 1, 1, 5, 5, 5, 1, 0, 1,\n",
    "       4, 1, 5, 5, 5, 5, 5, 1, 5, 1, 1, 1, 5, 4, 0, 5, 1, 5, 1, 2, 2, 0,\n",
    "       1, 1, 5, 5, 1, 5, 5, 5, 2, 4, 1, 5, 1, 5, 0, 5, 1, 2, 1, 5, 5, 2,\n",
    "       5, 5, 1, 2, 5, 4, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 5, 1, 3, 3, 5,\n",
    "       5, 5, 5, 1, 5, 5, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 1,\n",
    "       1, 5, 5, 1, 5, 5, 0, 1, 1, 3, 1, 5, 5, 1, 5, 5, 2, 5, 2, 5, 1, 1,\n",
    "       5, 5, 3, 4, 5, 5, 5, 1, 3, 1, 5, 5, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 5, 5, ..., 5, 3, 0],\n",
       "       [5, 5, 5, ..., 5, 0, 0],\n",
       "       [5, 5, 5, ..., 5, 0, 0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = pd.DataFrame(dfAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  378  379  380  381  \\\n",
       "0    5    5    5    5    5    5    5    5    5    5  ...    5    5    5    1   \n",
       "1    5    5    5    5    5    5    5    5    5    5  ...    5    0    5    1   \n",
       "2    5    5    5    5    5    5    5    5    5    5  ...    5    5    5    1   \n",
       "\n",
       "   382  383  384  385  386  387  \n",
       "0    1    1    5    5    3    0  \n",
       "1    0    1    0    5    0    0  \n",
       "2    3    1    5    5    0    0  \n",
       "\n",
       "[3 rows x 388 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  378  379  380  381  \\\n",
       "0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  ...  5.0  5.0  5.0  1.0   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN   \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   382  383  384  385  386  387  \n",
       "0    0  1.0  5.0  5.0  0.0  0.0  \n",
       "1    1  NaN  NaN  NaN  NaN  NaN  \n",
       "2    3  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[3 rows x 388 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.mode(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      2\n",
       "2      2\n",
       "3      2\n",
       "4      2\n",
       "      ..\n",
       "383    2\n",
       "384    2\n",
       "385    2\n",
       "386    2\n",
       "387    2\n",
       "Length: 388, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.mode(axis=0).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test2.shape[0]-dfAll.mode(axis=0).isnull().sum().sum()/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the calculation above, we know there are 10 instances that all three methods produce different predictions. So the majority vote would not work. For a problem like this with 6 classes, we need 7 different models to ensemble to make sure there is a majority vote result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work:\n",
    "#### improvement consider it is a imbalance dataset use class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
